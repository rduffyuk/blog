[{"content":"Why This Blog Exists After months of building a local AI infrastructure stack, I kept encountering the same problem: most AI content online is either too theoretical or too enterprise-focused.\nI wanted to share what actually works when you\u0026rsquo;re running LLMs on a consumer GPU - the real performance numbers, the gotchas, the configuration tweaks that made the difference.\nWhat I\u0026rsquo;ve Built So Far Here\u0026rsquo;s my current setup (as of October 2025):\nHardware GPU: NVIDIA RTX 4080 (16GB VRAM) CPU: Intel i9-13900KF RAM: 62GB DDR5 Storage: 2TB NVMe SSD Software Stack LLM Runtime: Ollama 0.12.3 (11 models) High-Performance Inference: vLLM 0.10.2 Knowledge Base: Obsidian vault with 504 indexed documents Semantic Search: ChromaDB with mxbai-embed-large-v1 Automation: Prefect workflows (journal generation, vault indexing) Observability: Jaeger + OpenTelemetry + Kafka on K3s Performance Achievements 24x faster inference with vLLM vs standard serving 32-600% speed improvement after Ollama 0.12.3 upgrade Zero-cost operation for unlimited queries \u0026lt; 1 second semantic search across 500+ documents Automated maintenance with Prefect flows every 30 minutes What You\u0026rsquo;ll Learn Here I\u0026rsquo;ll be covering:\nüöÄ Performance Tuning vLLM configuration for consumer GPUs Ollama optimization strategies CUDA memory management Model quantization trade-offs üèóÔ∏è Architecture Patterns Building automation pipelines with Prefect Semantic search with ChromaDB Kubernetes observability on K3s Integration patterns for local LLMs üìä Real Benchmarks Tokens/second across different models VRAM usage patterns Latency measurements Quality vs speed trade-offs üõ†Ô∏è Practical How-Tos Setting up specific tools Debugging common issues Configuration that actually matters Cost optimization strategies My Philosophy Local-first, automation-driven, measurably fast.\nI believe in:\nBuilding systems that maintain themselves Measuring everything that matters Sharing real data, not marketing claims Making professional AI accessible Coming Soon I\u0026rsquo;m working on deep-dives into:\nMy Phase 4 automation architecture vLLM 0.10.2 setup guide for RTX 4080 Obsidian ‚Üí ChromaDB indexing pipeline Multi-agent routing patterns Let\u0026rsquo;s Connect Follow along as I document this journey. Subscribe via RSS, or check out my GitHub for the code behind these systems.\nThis blog is generated from my Obsidian vault using an automated Hugo + Cloudflare Pages pipeline. Meta, right?\n","permalink":"http://localhost:1313/posts/welcome/","summary":"\u003ch2 id=\"why-this-blog-exists\"\u003eWhy This Blog Exists\u003c/h2\u003e\n\u003cp\u003eAfter months of building a local AI infrastructure stack, I kept encountering the same problem: \u003cstrong\u003emost AI content online is either too theoretical or too enterprise-focused\u003c/strong\u003e.\u003c/p\u003e\n\u003cp\u003eI wanted to share what actually works when you\u0026rsquo;re running LLMs on a consumer GPU - the real performance numbers, the gotchas, the configuration tweaks that made the difference.\u003c/p\u003e\n\u003ch2 id=\"what-ive-built-so-far\"\u003eWhat I\u0026rsquo;ve Built So Far\u003c/h2\u003e\n\u003cp\u003eHere\u0026rsquo;s my current setup (as of October 2025):\u003c/p\u003e","title":"Welcome to My AI Infrastructure Journey"},{"content":"About This Blog Hi, I\u0026rsquo;m Ryan Duffy, and this is my digital garden where I document my journey building a production-grade AI infrastructure on consumer hardware.\nWhat I\u0026rsquo;m Building I run a complete local AI stack on an RTX 4080:\n11 local LLM models via Ollama (Mistral, Qwen, DeepSeek) vLLM server for high-performance inference Kubernetes observability (Jaeger, OpenTelemetry, Kafka) ChromaDB semantic search with 504 indexed documents Automated workflows using Prefect Why This Blog? Most AI content is either:\nTheoretical tutorials that don\u0026rsquo;t show real performance Enterprise solutions requiring $10k/month budgets Hobby projects that don\u0026rsquo;t scale I\u0026rsquo;m bridging the gap - showing how to build professional AI infrastructure on a ¬£1,500 GPU that delivers production-quality results.\nWhat You\u0026rsquo;ll Learn Real benchmarks: Actual tokens/sec, VRAM usage, latency measurements Configuration deep-dives: The settings that actually matter Automation patterns: How to build systems that maintain themselves Cost optimization: Getting $0/month inference with enterprise quality My Stack Hardware: RTX 4080 (16GB), i9-13900KF, 62GB RAM Software: Ubuntu 22.04, K3s, Ollama 0.12.3, vLLM, PyTorch Workflow: Obsidian vault ‚Üí Prefect automation ‚Üí ChromaDB indexing Philosophy: Local-first, automation-driven, measurably fast\nConnect GitHub: github.com/rduffyuk RSS: Subscribe to stay updated on new posts Last updated: October 2025\n","permalink":"http://localhost:1313/about/","summary":"\u003ch1 id=\"about-this-blog\"\u003eAbout This Blog\u003c/h1\u003e\n\u003cp\u003eHi, I\u0026rsquo;m \u003cstrong\u003eRyan Duffy\u003c/strong\u003e, and this is my digital garden where I document my journey building a production-grade AI infrastructure on consumer hardware.\u003c/p\u003e\n\u003ch2 id=\"what-im-building\"\u003eWhat I\u0026rsquo;m Building\u003c/h2\u003e\n\u003cp\u003eI run a complete local AI stack on an RTX 4080:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003e11 local LLM models\u003c/strong\u003e via Ollama (Mistral, Qwen, DeepSeek)\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003evLLM server\u003c/strong\u003e for high-performance inference\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eKubernetes observability\u003c/strong\u003e (Jaeger, OpenTelemetry, Kafka)\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eChromaDB semantic search\u003c/strong\u003e with 504 indexed documents\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eAutomated workflows\u003c/strong\u003e using Prefect\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id=\"why-this-blog\"\u003eWhy This Blog?\u003c/h2\u003e\n\u003cp\u003eMost AI content is either:\u003c/p\u003e","title":"About"}]