<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>The AI Awakening: Breaking Free from Context Limits | Ryan Duffy - AI Infrastructure &amp; Local LLM Journey</title>
<meta name="keywords" content="ollama, local-llm, deepseek, rtx-4080, ai-infrastructure, collaboration, context-windows">
<meta name="description" content="From context window frustration to local control. September 18-19, 2025 - a weekend working with Claude Code to install 17 local models, discovering what it means to own your AI conversations while working a full-time job.">
<meta name="author" content="Ryan Duffy">
<link rel="canonical" href="https://blog.rduffy.uk/posts/season-1-episode-3-ai-awakening/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.a090830a421002426baafbd314e38f149d77b4c48a12ee9312700d770b27fb26.css" integrity="sha256-oJCDCkIQAkJrqvvTFOOPFJ13tMSKEu6TEnANdwsn&#43;yY=" rel="preload stylesheet" as="style">
<link rel="icon" href="https://blog.rduffy.uk/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="https://blog.rduffy.uk/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://blog.rduffy.uk/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://blog.rduffy.uk/apple-touch-icon.png">
<link rel="mask-icon" href="https://blog.rduffy.uk/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="en" href="https://blog.rduffy.uk/posts/season-1-episode-3-ai-awakening/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --code-block-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript><link rel="icon" type="image/svg+xml" href="/favicon.svg"><style>
   
  .mermaid-diagram {
    margin: 2.5rem 0;
    padding: 1.5rem;
    background: transparent;
    border: none;
    border-radius: 8px;
    text-align: center;
  }

   
  .mermaid-diagram img {
    max-width: 100%;
    height: auto;
    min-height: 500px;
    display: block;
    margin: 0 auto;
    image-rendering: -webkit-optimize-contrast;
    image-rendering: crisp-edges;
  }

   
  @media (max-width: 768px) {
    .mermaid-diagram {
      padding: 1rem;
      margin: 1.5rem -1rem;
    }
    .mermaid-diagram img {
      min-height: 400px;
    }
  }
</style>
<meta property="og:url" content="https://blog.rduffy.uk/posts/season-1-episode-3-ai-awakening/">
  <meta property="og:site_name" content="Ryan Duffy - AI Infrastructure & Local LLM Journey">
  <meta property="og:title" content="The AI Awakening: Breaking Free from Context Limits">
  <meta property="og:description" content="From context window frustration to local control. September 18-19, 2025 - a weekend working with Claude Code to install 17 local models, discovering what it means to own your AI conversations while working a full-time job.">
  <meta property="og:locale" content="en-gb">
  <meta property="og:type" content="article">
    <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2025-10-05T00:00:00+00:00">
    <meta property="article:modified_time" content="2025-10-05T00:00:00+00:00">
    <meta property="article:tag" content="Ollama">
    <meta property="article:tag" content="Local-Llm">
    <meta property="article:tag" content="Deepseek">
    <meta property="article:tag" content="Rtx-4080">
    <meta property="article:tag" content="Ai-Infrastructure">
    <meta property="article:tag" content="Collaboration">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="The AI Awakening: Breaking Free from Context Limits">
<meta name="twitter:description" content="From context window frustration to local control. September 18-19, 2025 - a weekend working with Claude Code to install 17 local models, discovering what it means to own your AI conversations while working a full-time job.">


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Posts",
      "item": "https://blog.rduffy.uk/posts/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "The AI Awakening: Breaking Free from Context Limits",
      "item": "https://blog.rduffy.uk/posts/season-1-episode-3-ai-awakening/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "The AI Awakening: Breaking Free from Context Limits",
  "name": "The AI Awakening: Breaking Free from Context Limits",
  "description": "From context window frustration to local control. September 18-19, 2025 - a weekend working with Claude Code to install 17 local models, discovering what it means to own your AI conversations while working a full-time job.",
  "keywords": [
    "ollama", "local-llm", "deepseek", "rtx-4080", "ai-infrastructure", "collaboration", "context-windows"
  ],
  "articleBody": "Episode 3: The AI Awakening - Breaking Free from Context Limits Series: Season 1 - From Zero to Automated Infrastructure Episode: 3 of 8 Dates: September 18-19, 2025 (Weekend) Reading Time: 8 minutes\nThe Context Window Problem (Again) By Wednesday, September 18, ConvoCanvas was working. The MVP could parse conversations and generate content ideas. But the original problem that started this whole journey? Still unsolved.\n❌ Error: Context window overflow. This conversation is too long to continue. Auto-compacting conversation... I was still hitting context limits. Still losing conversation history. Still starting over every time Claude Code or ChatGPT hit their limits.\nConvoCanvas could organize the past conversations, but it couldn’t prevent me from hitting limits on new conversations.\nThe real problem wasn’t storage - it was conversation continuity.\nThe Realization: I Need My Own Models Vault Evidence: Sept 18 reflection journal (319 lines) documents the full day of Ollama research, installation, and setup working with Claude Code. The journal shows activity from 6:30 AM through 11:00 PM - a complete weekend day focused on this work.\nWednesday evening after work at BT, I researched local LLMs. The issue wasn’t cost (I was using Claude Code, not paying per API call). The issue was control.\nWhat I couldn’t control with external services:\n❌ Context window limits - Hit 200K tokens? Start over. ❌ Conversation persistence - Can’t continue yesterday’s deep dive ❌ Model availability - Service down? Can’t work. ❌ Privacy concerns - Every conversation goes to external servers ❌ Experimentation freedom - Can’t test ideas without worrying about limits What I needed:\n✅ Configurable context (choose models with appropriate limits) ✅ Persistent conversations (save and resume anytime) ✅ 24/7 availability (works offline) ✅ Complete privacy (never leaves my machine) ✅ Unlimited experimentation (no external throttling or billing) I needed local inference. I needed Ollama.\nReality check: Local models still have context limits (Llama 3.1: 128K tokens, DeepSeek R1: 32K tokens). But I could choose the right model for each task and save/resume conversations across sessions. The win wasn’t unlimited context - it was control over the context.\n┌─────────────────────────────────────────────────────┐ │ External Services vs Local Control (Sept 18) │ └─────────────────────────────────────────────────────┘ EXTERNAL (Claude/ChatGPT) LOCAL (Ollama) ───────────────────────────── ────────────────── ❌ Hard context limits → ✅ Configurable limits ❌ Forced restarts → ✅ Save/resume anytime ❌ Service dependency → ✅ Offline capable ❌ External logging → ✅ Complete privacy ❌ Rate limiting → ✅ Unlimited local use TRADE-OFF: Claude reasoning quality \u0026gt; Local model quality BUT: Local persistence \u0026gt; Forced restarts September 19, Morning - Installation (Working with Claude) Vault Evidence: Sept 18 journal shows “Ollama + DeepSeek R1 installation”, “Model Performance”, “Concurrent Loading”, “Timeout Tuning” - confirming Ollama work happened Sept 18-19.\nSaturday morning. Time to install Ollama.\nWorking with Claude Code throughout the day, I researched hardware requirements, model selection, and performance targets.\nClaude and I worked through:\nHardware compatibility (RTX 4080, 16GB VRAM) Model quantization (GGUF formats) Concurrent model loading strategies Context window comparisons # Install Ollama (Claude provided the command) curl -fsSL https://ollama.com/install.sh | sh # Verify GPU access ollama run llama3.1 # Output: Using NVIDIA RTX 4080, 16GB VRAM # Model loaded in 2.3 seconds IT WORKED.\nThe RTX 4080 was humming. VRAM usage: 6.2GB for Llama 3.1 8B. Plenty of headroom.\nMid-Morning - Model Collection Working with Claude to understand which models to install, I started pulling models:\n# Reasoning specialist (Claude\u0026#39;s recommendation) ollama pull deepseek-r1:7b # General purpose (fastest) ollama pull mistral:7b-instruct # Meta\u0026#39;s latest ollama pull llama3.1:8b # Code specialist ollama pull codellama:7b # Uncensored variant (for creative tasks) ollama pull nous-hermes-2:latest # Compact model (2B for quick tasks) ollama pull phi-3:mini Total download: 42GB Installation time: 35 minutes Models available: 6\nBut Claude suggested more models for different use cases. By afternoon, I had 17 models installed.\nVault Evidence: Sept 18 journal confirms “DeepSeek R1:7b achieves 71.61 tokens/sec on RTX 4080” - showing actual performance testing happened.\nModel Size Purpose VRAM Context Window DeepSeek R1 7B Reasoning \u0026 analysis 4.2GB 32K tokens Mistral Instruct 7B General chat 4.1GB 32K tokens Llama 3.1 8B Latest Meta model 4.8GB 128K tokens CodeLlama 7B Code generation 4.3GB 16K tokens Nous Hermes 2 7B Creative writing 4.2GB 8K tokens Phi-3 Mini 2B Quick tasks 1.4GB 4K tokens Qwen 2.5 7B Multilingual 4.5GB 32K tokens Neural Chat 7B Conversational 4.0GB 8K tokens Orca Mini 3B Compact reasoning 1.9GB 2K tokens Vicuna 7B Research assistant 4.4GB 2K tokens WizardCoder 7B Code debugging 4.3GB 16K tokens Zephyr 7B Instruction following 4.1GB 8K tokens OpenHermes 7B General purpose 4.2GB 8K tokens Starling 7B Advanced reasoning 4.6GB 8K tokens Solar 10.7B Performance leader 6.8GB 4K tokens Yi-34B 34B (quantized) Heavy lifting 12.1GB 4K tokens Mixtral 8x7B 47B (quantized) Mixture of experts 14.2GB 32K tokens The RTX 4080 could handle them all. (Just not simultaneously.)\n┌──────────────────────────────────────────────────┐ │ RTX 4080 Model Loading (Sept 19, Morning) │ │ (Optimized with Claude\u0026#39;s help) │ └──────────────────────────────────────────────────┘ VRAM: 16GB Total ├─ Llama 3.1 (8B): 4.8GB [████████░░░░░░░░] 30% ├─ DeepSeek R1 (7B): 4.2GB [███████░░░░░░░░░] 26% ├─ Mixtral (47B): 14.2GB [██████████████░░] 89% └─ 3x Concurrent: 12.4GB [████████████░░░░] 78% Optimal Configuration (Claude\u0026#39;s analysis): • 3 models @ 7B each = 12.4GB (sweet spot) • Switching time: 2-4 seconds • Response time: 1.8-2.3 seconds avg • Total models available: 17 Afternoon - Understanding the Potential With 17 models installed, Claude and I explored what this local setup actually meant.\nThe Research Had Shown:\nLlama 3.1: 128K token context window DeepSeek R1: 32K token context window All conversations stay on my machine No forced restarts from external services I hadn’t extensively tested it yet, but the capability was there. Unlike Claude Code or ChatGPT, which force conversation compaction when you hit limits, Ollama conversations could theoretically continue as long as VRAM allowed.\nThe Real Win Wasn’t Unlimited Context - it was something else entirely.\nEvening - The Control Realization The breakthrough wasn’t about having infinite context. It was about owning the conversation.\nWhat Changed:\nBefore: Hit 200K tokens → System forces auto-compact → Lose nuance After: Choose model with appropriate context → Manage memory myself → Decide when to move on The Freedom I Gained:\nExternal Service: \u0026#34;You\u0026#39;ve hit the limit. Auto-compacting...\u0026#34; Local Ollama: \u0026#34;12.4GB VRAM used. Continue or switch models?\u0026#34; External Service: \u0026#34;Service unavailable. Try again later.\u0026#34; Local Ollama: \u0026#34;Offline? No problem. Still running.\u0026#34; External Service: \u0026#34;Conversation logged to our servers.\u0026#34; Local Ollama: \u0026#34;Everything stays on your machine.\u0026#34; I wasn’t escaping context limits - I was escaping forced decisions about MY conversations.\nThat was the real breakthrough.\nSunday Morning - The Supervisor Pattern Vault Evidence: Sept 18 journal confirms “Supervisor Pattern Success”, “Intelligent Routing”, “Context Engineering” work.\nWith 17 models available, Claude and I built an orchestrator to route tasks to the best model:\n# Designed collaboratively with Claude Code class ModelSupervisor: def __init__(self): self.models = { \u0026#34;reasoning\u0026#34;: \u0026#34;deepseek-r1:7b\u0026#34;, \u0026#34;general\u0026#34;: \u0026#34;mistral:7b-instruct\u0026#34;, \u0026#34;code\u0026#34;: \u0026#34;codellama:7b\u0026#34;, \u0026#34;fast\u0026#34;: \u0026#34;phi-3:mini\u0026#34;, \u0026#34;creative\u0026#34;: \u0026#34;nous-hermes-2:latest\u0026#34;, \u0026#34;long_context\u0026#34;: \u0026#34;llama3.1:8b\u0026#34; # 128K context! } def route_task(self, task_type: str, prompt: str) -\u0026gt; str: \u0026#34;\u0026#34;\u0026#34;Route task to optimal model.\u0026#34;\u0026#34;\u0026#34; model = self.models.get(task_type, self.models[\u0026#34;general\u0026#34;]) response = requests.post( \u0026#34;http://localhost:11434/api/generate\u0026#34;, json={\u0026#34;model\u0026#34;: model, \u0026#34;prompt\u0026#34;: prompt} ) return response.json()[\u0026#34;response\u0026#34;] ┌────────────────────────────────────────────────┐ │ Supervisor Pattern Routing (Sept 19, AM) │ │ (Designed with Claude Code\u0026#39;s help) │ └────────────────────────────────────────────────┘ ┌─────────────────┐ │ Supervisor │ │ Decides │ └────────┬────────┘ │ ┌────────────────┼────────────────┐ │ │ │ ▼ ▼ ▼ ┌──────────┐ ┌──────────┐ ┌──────────┐ │ DeepSeek │ │ CodeLlama│ │ Llama │ │ R1 │ │ 7B │ │ 3.1 8B │ └──────────┘ └──────────┘ └──────────┘ Reasoning Code Gen Long Context 32K tokens 16K tokens 128K tokens ROUTING LOGIC: • Code review → CodeLlama (specialized) • Long analysis → Llama 3.1 (128K context) • Deep reasoning → DeepSeek R1 (quality) • Quick answers → Phi-3 Mini (speed) The system could now self-optimize based on context needs.\nThe Reality: A Weekend Project While Working Full-Time Vault Evidence: Sept 18 journal shows continuous activity from 6:30 AM through 11:00 PM - a full weekend day of focused work.\nThis wasn’t a quick evening project. The Sept 18 reflection journal shows:\nMorning (6:30 AM): Starting automation systems Afternoon: Ollama installation and model collection Evening (through 11:00 PM): Supervisor pattern, testing, integration But it was also broken up by life:\nWork at BT during the week (Monday-Friday) Saturday-Sunday: Personal project time Breaks between intense coding sessions Real life happening around the development The journal shows the reality: This was focused weekend work, not a corporate “sprint”. Personal time, personal pace, personal project.\nWhat Worked Working with Claude Code: This supervisor pattern, model selection strategy, VRAM optimization - all designed collaboratively. Claude brought patterns, I brought context, together we built something better.\nOllama’s Model Management: Single command to pull, update, or remove models. No Docker containers, no config files, no complexity.\nContext Persistence: Finally solved the original Day Zero problem - no more losing conversation history!\nGPU Performance: RTX 4080 handled everything I threw at it. 16GB VRAM was the sweet spot for running multiple 7B models.\nPrivacy \u0026 Control: All conversations stay local. No external logging. Complete ownership of my AI interactions.\nWhat Still Sucked Model Switching Latency: Loading a new model: 2-4 seconds. Not terrible, but noticeable when switching frequently.\nVRAM Juggling: Can’t run Mixtral 8x7B (14.2GB) alongside anything else. Had to be strategic about which models stayed loaded.\nQuality Variance: Some models (Phi-3 Mini) were fast but shallow. Others (DeepSeek R1) were brilliant but slower. Required testing to find the right fit.\nStill Need Claude Code: Local models are good, but Claude Code’s reasoning is still unmatched for complex tasks. Ollama complements, doesn’t replace.\nThe Numbers (Sept 18-19, 2025) Metric Value Time Spent Weekend (Saturday-Sunday) Work Hours ~15 hours (split across 2 days) Models Installed 17 Total Download Size 78GB VRAM Available 16GB (RTX 4080) Context Limit Freedom Unlimited (hardware-bound) Average Response Time 2.1 seconds Concurrent Models 3 (12.4GB VRAM) External Dependencies Eliminated ★ Insight ───────────────────────────────────── The Freedom of Local Inference:\nSwitching to local LLMs wasn’t about cost - it was about solving the original problem:\nOwnership - You control when conversations end, not a service Privacy - Conversations never leave the machine Offline capability - No internet required Experimentation freedom - Iterate without external throttling Learning - Direct access to model internals, VRAM, performance tuning Choice - Pick models with context windows matching your needs This was built working WITH Claude Code - collaborative AI development where human understanding + AI patterns created better solutions than either alone.\nThe cost savings ($0/year vs potential API costs) were a bonus. The real win was control over the context window.\nDay Zero’s context window problem? Not eliminated - but now under MY control. ─────────────────────────────────────────────────\nWhat I Learned 1. Weekend projects fit around full-time work Saturday-Sunday intensive work. Monday-Friday back to day job. This is the reality of personal projects.\n2. Collaboration makes better solutions Claude Code + my domain knowledge = supervisor pattern we wouldn’t have designed individually.\n3. Control over context \u003e raw performance Having the option to manage conversation memory yourself is more valuable than slightly faster responses from a service that forces compaction.\n4. Privacy enables experimentation Knowing conversations stay local removes psychological barriers to trying wild ideas.\n5. Local doesn’t mean isolated Ollama + Claude Code = best of both worlds. Use local for persistent work, cloud for complex reasoning.\nWhat’s Next Ollama was running. I had local control over my AI conversations. But the system was generating responses faster than I could organize them.\nWorking with Ollama over the next few days would generate hundreds more conversation files. By September 20, I’d need a way to search them all.\nThat’s when ChromaDB and semantic search would enter the picture.\nThis is Episode 3 of “Season 1: From Zero to Automated Infrastructure” - documenting the weekend that solved the context window problem with local AI.\nPrevious Episode: Building the Foundation: Vault Creation to MVP Next Episode: ChromaDB Weekend: From 504 to 24,916 Documents Complete Series: Season 1 Mapping Report\n",
  "wordCount" : "1976",
  "inLanguage": "en",
  "datePublished": "2025-10-05T00:00:00Z",
  "dateModified": "2025-10-05T00:00:00Z",
  "author":{
    "@type": "Person",
    "name": "Ryan Duffy"
  },
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://blog.rduffy.uk/posts/season-1-episode-3-ai-awakening/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "Ryan Duffy - AI Infrastructure \u0026 Local LLM Journey",
    "logo": {
      "@type": "ImageObject",
      "url": "https://blog.rduffy.uk/favicon.ico"
    }
  }
}
</script>
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://blog.rduffy.uk/" accesskey="h" title="Ryan Duffy - AI Infrastructure &amp; Local LLM Journey (Alt + H)">Ryan Duffy - AI Infrastructure &amp; Local LLM Journey</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)" aria-label="Toggle theme">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="https://blog.rduffy.uk/posts/" title="Posts">
                    <span>Posts</span>
                </a>
            </li>
            <li>
                <a href="https://blog.rduffy.uk/tags/" title="Tags">
                    <span>Tags</span>
                </a>
            </li>
            <li>
                <a href="https://blog.rduffy.uk/about/" title="About">
                    <span>About</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    <div class="breadcrumbs"><a href="https://blog.rduffy.uk/">Home</a>&nbsp;»&nbsp;<a href="https://blog.rduffy.uk/posts/">Posts</a></div>
    <h1 class="post-title entry-hint-parent">
      The AI Awakening: Breaking Free from Context Limits
    </h1>
    <div class="post-meta"><span title='2025-10-05 00:00:00 +0000 UTC'>October 5, 2025</span>&nbsp;·&nbsp;<span>10 min</span>&nbsp;·&nbsp;<span>1976 words</span>&nbsp;·&nbsp;<span>Ryan Duffy</span>

</div>
  </header> <div class="toc">
    <details >
        <summary accesskey="c" title="(Alt + C)">
            <span class="details">Table of Contents</span>
        </summary>

        <div class="inner"><nav id="TableOfContents">
  <ul>
    <li><a href="#the-context-window-problem-again">The Context Window Problem (Again)</a></li>
    <li><a href="#the-realization-i-need-my-own-models">The Realization: I Need My Own Models</a></li>
    <li><a href="#september-19-morning---installation-working-with-claude">September 19, Morning - Installation (Working with Claude)</a></li>
    <li><a href="#mid-morning---model-collection">Mid-Morning - Model Collection</a></li>
    <li><a href="#afternoon---understanding-the-potential">Afternoon - Understanding the Potential</a></li>
    <li><a href="#evening---the-control-realization">Evening - The Control Realization</a></li>
    <li><a href="#sunday-morning---the-supervisor-pattern">Sunday Morning - The Supervisor Pattern</a></li>
    <li><a href="#the-reality-a-weekend-project-while-working-full-time">The Reality: A Weekend Project While Working Full-Time</a></li>
    <li><a href="#what-worked">What Worked</a></li>
    <li><a href="#what-still-sucked">What Still Sucked</a></li>
    <li><a href="#the-numbers-sept-18-19-2025">The Numbers (Sept 18-19, 2025)</a></li>
    <li><a href="#what-i-learned">What I Learned</a></li>
    <li><a href="#whats-next">What&rsquo;s Next</a></li>
  </ul>
</nav>
        </div>
    </details>
</div>

  <div class="post-content"><h1 id="episode-3-the-ai-awakening---breaking-free-from-context-limits">Episode 3: The AI Awakening - Breaking Free from Context Limits<a hidden class="anchor" aria-hidden="true" href="#episode-3-the-ai-awakening---breaking-free-from-context-limits">#</a></h1>
<p><strong>Series</strong>: Season 1 - From Zero to Automated Infrastructure
<strong>Episode</strong>: 3 of 8
<strong>Dates</strong>: September 18-19, 2025 (Weekend)
<strong>Reading Time</strong>: 8 minutes</p>
<hr>
<h2 id="the-context-window-problem-again">The Context Window Problem (Again)<a hidden class="anchor" aria-hidden="true" href="#the-context-window-problem-again">#</a></h2>
<p>By Wednesday, September 18, ConvoCanvas was working. The MVP could parse conversations and generate content ideas. But the original problem that started this whole journey? <strong>Still unsolved.</strong></p>
<div class="highlight">
  <pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code>❌ Error: Context window overflow. This conversation is too long to continue.
Auto-compacting conversation...</code></pre>
</div>
<p>I was still hitting context limits. Still losing conversation history. Still starting over every time Claude Code or ChatGPT hit their limits.</p>
<p>ConvoCanvas could organize the <strong>past</strong> conversations, but it couldn&rsquo;t prevent me from hitting limits on <strong>new</strong> conversations.</p>
<p>The real problem wasn&rsquo;t storage - it was <strong>conversation continuity</strong>.</p>
<h2 id="the-realization-i-need-my-own-models">The Realization: I Need My Own Models<a hidden class="anchor" aria-hidden="true" href="#the-realization-i-need-my-own-models">#</a></h2>
<p><em>Vault Evidence: Sept 18 reflection journal (319 lines) documents the full day of Ollama research, installation, and setup working with Claude Code. The journal shows activity from 6:30 AM through 11:00 PM - a complete weekend day focused on this work.</em></p>
<p>Wednesday evening after work at BT, I researched local LLMs. The issue wasn&rsquo;t cost (I was using Claude Code, not paying per API call). The issue was <strong>control</strong>.</p>
<p><strong>What I couldn&rsquo;t control with external services</strong>:</p>
<ul>
<li>❌ <strong>Context window limits</strong> - Hit 200K tokens? Start over.</li>
<li>❌ <strong>Conversation persistence</strong> - Can&rsquo;t continue yesterday&rsquo;s deep dive</li>
<li>❌ <strong>Model availability</strong> - Service down? Can&rsquo;t work.</li>
<li>❌ <strong>Privacy concerns</strong> - Every conversation goes to external servers</li>
<li>❌ <strong>Experimentation freedom</strong> - Can&rsquo;t test ideas without worrying about limits</li>
</ul>
<p><strong>What I needed</strong>:</p>
<ul>
<li>✅ <strong>Configurable context</strong> (choose models with appropriate limits)</li>
<li>✅ <strong>Persistent conversations</strong> (save and resume anytime)</li>
<li>✅ <strong>24/7 availability</strong> (works offline)</li>
<li>✅ <strong>Complete privacy</strong> (never leaves my machine)</li>
<li>✅ <strong>Unlimited experimentation</strong> (no external throttling or billing)</li>
</ul>
<p>I needed local inference. I needed <strong>Ollama</strong>.</p>
<p><strong>Reality check</strong>: Local models still have context limits (Llama 3.1: 128K tokens, DeepSeek R1: 32K tokens). But I could <strong>choose</strong> the right model for each task and <strong>save/resume</strong> conversations across sessions. The win wasn&rsquo;t unlimited context - it was <strong>control over the context</strong>.</p>
<div class="highlight">
  <pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code>┌─────────────────────────────────────────────────────┐
│     External Services vs Local Control (Sept 18)   │
└─────────────────────────────────────────────────────┘

EXTERNAL (Claude/ChatGPT)          LOCAL (Ollama)
─────────────────────────────      ──────────────────
❌ Hard context limits        →    ✅ Configurable limits
❌ Forced restarts            →    ✅ Save/resume anytime
❌ Service dependency         →    ✅ Offline capable
❌ External logging           →    ✅ Complete privacy
❌ Rate limiting              →    ✅ Unlimited local use

TRADE-OFF:
Claude reasoning quality &amp;gt; Local model quality
BUT: Local persistence &amp;gt; Forced restarts</code></pre>
</div>
<h2 id="september-19-morning---installation-working-with-claude">September 19, Morning - Installation (Working with Claude)<a hidden class="anchor" aria-hidden="true" href="#september-19-morning---installation-working-with-claude">#</a></h2>
<p><em>Vault Evidence: Sept 18 journal shows &ldquo;Ollama + DeepSeek R1 installation&rdquo;, &ldquo;Model Performance&rdquo;, &ldquo;Concurrent Loading&rdquo;, &ldquo;Timeout Tuning&rdquo; - confirming Ollama work happened Sept 18-19.</em></p>
<p>Saturday morning. Time to install Ollama.</p>
<p>Working with Claude Code throughout the day, I researched hardware requirements, model selection, and performance targets.</p>
<p><strong>Claude and I worked through</strong>:</p>
<ul>
<li>Hardware compatibility (RTX 4080, 16GB VRAM)</li>
<li>Model quantization (GGUF formats)</li>
<li>Concurrent model loading strategies</li>
<li>Context window comparisons</li>
</ul>
<div class="highlight">
  <pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code># Install Ollama (Claude provided the command)
curl -fsSL https://ollama.com/install.sh | sh

# Verify GPU access
ollama run llama3.1
# Output: Using NVIDIA RTX 4080, 16GB VRAM
# Model loaded in 2.3 seconds</code></pre>
</div>
<p><strong>IT WORKED.</strong></p>
<p>The RTX 4080 was humming. VRAM usage: 6.2GB for Llama 3.1 8B. Plenty of headroom.</p>
<h2 id="mid-morning---model-collection">Mid-Morning - Model Collection<a hidden class="anchor" aria-hidden="true" href="#mid-morning---model-collection">#</a></h2>
<p>Working with Claude to understand which models to install, I started pulling models:</p>
<div class="highlight">
  <pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code># Reasoning specialist (Claude&amp;#39;s recommendation)
ollama pull deepseek-r1:7b

# General purpose (fastest)
ollama pull mistral:7b-instruct

# Meta&amp;#39;s latest
ollama pull llama3.1:8b

# Code specialist
ollama pull codellama:7b

# Uncensored variant (for creative tasks)
ollama pull nous-hermes-2:latest

# Compact model (2B for quick tasks)
ollama pull phi-3:mini</code></pre>
</div>
<p><strong>Total download</strong>: 42GB
<strong>Installation time</strong>: 35 minutes
<strong>Models available</strong>: 6</p>
<p>But Claude suggested more models for different use cases. By afternoon, I had <strong>17 models</strong> installed.</p>
<p><em>Vault Evidence: Sept 18 journal confirms &ldquo;DeepSeek R1:7b achieves 71.61 tokens/sec on RTX 4080&rdquo; - showing actual performance testing happened.</em></p>
<table>
  <thead>
      <tr>
          <th>Model</th>
          <th>Size</th>
          <th>Purpose</th>
          <th>VRAM</th>
          <th>Context Window</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td><strong>DeepSeek R1</strong></td>
          <td>7B</td>
          <td>Reasoning &amp; analysis</td>
          <td>4.2GB</td>
          <td>32K tokens</td>
      </tr>
      <tr>
          <td><strong>Mistral Instruct</strong></td>
          <td>7B</td>
          <td>General chat</td>
          <td>4.1GB</td>
          <td>32K tokens</td>
      </tr>
      <tr>
          <td><strong>Llama 3.1</strong></td>
          <td>8B</td>
          <td>Latest Meta model</td>
          <td>4.8GB</td>
          <td>128K tokens</td>
      </tr>
      <tr>
          <td><strong>CodeLlama</strong></td>
          <td>7B</td>
          <td>Code generation</td>
          <td>4.3GB</td>
          <td>16K tokens</td>
      </tr>
      <tr>
          <td><strong>Nous Hermes 2</strong></td>
          <td>7B</td>
          <td>Creative writing</td>
          <td>4.2GB</td>
          <td>8K tokens</td>
      </tr>
      <tr>
          <td><strong>Phi-3 Mini</strong></td>
          <td>2B</td>
          <td>Quick tasks</td>
          <td>1.4GB</td>
          <td>4K tokens</td>
      </tr>
      <tr>
          <td><strong>Qwen 2.5</strong></td>
          <td>7B</td>
          <td>Multilingual</td>
          <td>4.5GB</td>
          <td>32K tokens</td>
      </tr>
      <tr>
          <td><strong>Neural Chat</strong></td>
          <td>7B</td>
          <td>Conversational</td>
          <td>4.0GB</td>
          <td>8K tokens</td>
      </tr>
      <tr>
          <td><strong>Orca Mini</strong></td>
          <td>3B</td>
          <td>Compact reasoning</td>
          <td>1.9GB</td>
          <td>2K tokens</td>
      </tr>
      <tr>
          <td><strong>Vicuna</strong></td>
          <td>7B</td>
          <td>Research assistant</td>
          <td>4.4GB</td>
          <td>2K tokens</td>
      </tr>
      <tr>
          <td><strong>WizardCoder</strong></td>
          <td>7B</td>
          <td>Code debugging</td>
          <td>4.3GB</td>
          <td>16K tokens</td>
      </tr>
      <tr>
          <td><strong>Zephyr</strong></td>
          <td>7B</td>
          <td>Instruction following</td>
          <td>4.1GB</td>
          <td>8K tokens</td>
      </tr>
      <tr>
          <td><strong>OpenHermes</strong></td>
          <td>7B</td>
          <td>General purpose</td>
          <td>4.2GB</td>
          <td>8K tokens</td>
      </tr>
      <tr>
          <td><strong>Starling</strong></td>
          <td>7B</td>
          <td>Advanced reasoning</td>
          <td>4.6GB</td>
          <td>8K tokens</td>
      </tr>
      <tr>
          <td><strong>Solar</strong></td>
          <td>10.7B</td>
          <td>Performance leader</td>
          <td>6.8GB</td>
          <td>4K tokens</td>
      </tr>
      <tr>
          <td><strong>Yi-34B</strong></td>
          <td>34B (quantized)</td>
          <td>Heavy lifting</td>
          <td>12.1GB</td>
          <td>4K tokens</td>
      </tr>
      <tr>
          <td><strong>Mixtral 8x7B</strong></td>
          <td>47B (quantized)</td>
          <td>Mixture of experts</td>
          <td>14.2GB</td>
          <td>32K tokens</td>
      </tr>
  </tbody>
</table>
<p><strong>The RTX 4080 could handle them all.</strong> (Just not simultaneously.)</p>
<div class="highlight">
  <pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code>┌──────────────────────────────────────────────────┐
│    RTX 4080 Model Loading (Sept 19, Morning)    │
│         (Optimized with Claude&amp;#39;s help)           │
└──────────────────────────────────────────────────┘

VRAM: 16GB Total
├─ Llama 3.1 (8B):    4.8GB  [████████░░░░░░░░] 30%
├─ DeepSeek R1 (7B):  4.2GB  [███████░░░░░░░░░] 26%
├─ Mixtral (47B):    14.2GB  [██████████████░░] 89%
└─ 3x Concurrent:    12.4GB  [████████████░░░░] 78%

Optimal Configuration (Claude&amp;#39;s analysis):
• 3 models @ 7B each = 12.4GB (sweet spot)
• Switching time: 2-4 seconds
• Response time: 1.8-2.3 seconds avg
• Total models available: 17</code></pre>
</div>
<h2 id="afternoon---understanding-the-potential">Afternoon - Understanding the Potential<a hidden class="anchor" aria-hidden="true" href="#afternoon---understanding-the-potential">#</a></h2>
<p>With 17 models installed, Claude and I explored what this local setup actually meant.</p>
<p><strong>The Research Had Shown</strong>:</p>
<ul>
<li>Llama 3.1: 128K token context window</li>
<li>DeepSeek R1: 32K token context window</li>
<li>All conversations stay on my machine</li>
<li>No forced restarts from external services</li>
</ul>
<p><strong>I hadn&rsquo;t extensively tested it yet</strong>, but the capability was there. Unlike Claude Code or ChatGPT, which force conversation compaction when you hit limits, Ollama conversations could theoretically continue as long as VRAM allowed.</p>
<p><strong>The Real Win Wasn&rsquo;t Unlimited Context</strong> - it was something else entirely.</p>
<h2 id="evening---the-control-realization">Evening - The Control Realization<a hidden class="anchor" aria-hidden="true" href="#evening---the-control-realization">#</a></h2>
<p>The breakthrough wasn&rsquo;t about having infinite context. It was about <strong>owning the conversation</strong>.</p>
<p><strong>What Changed</strong>:</p>
<ul>
<li><strong>Before</strong>: Hit 200K tokens → System forces auto-compact → Lose nuance</li>
<li><strong>After</strong>: Choose model with appropriate context → Manage memory myself → Decide when to move on</li>
</ul>
<p><strong>The Freedom I Gained</strong>:</p>
<div class="highlight">
  <pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code>External Service:  &amp;#34;You&amp;#39;ve hit the limit. Auto-compacting...&amp;#34;
Local Ollama:      &amp;#34;12.4GB VRAM used. Continue or switch models?&amp;#34;

External Service:  &amp;#34;Service unavailable. Try again later.&amp;#34;
Local Ollama:      &amp;#34;Offline? No problem. Still running.&amp;#34;

External Service:  &amp;#34;Conversation logged to our servers.&amp;#34;
Local Ollama:      &amp;#34;Everything stays on your machine.&amp;#34;</code></pre>
</div>
<p>I wasn&rsquo;t escaping context limits - I was <strong>escaping forced decisions about MY conversations</strong>.</p>
<p><strong>That</strong> was the real breakthrough.</p>
<h2 id="sunday-morning---the-supervisor-pattern">Sunday Morning - The Supervisor Pattern<a hidden class="anchor" aria-hidden="true" href="#sunday-morning---the-supervisor-pattern">#</a></h2>
<p><em>Vault Evidence: Sept 18 journal confirms &ldquo;Supervisor Pattern Success&rdquo;, &ldquo;Intelligent Routing&rdquo;, &ldquo;Context Engineering&rdquo; work.</em></p>
<p>With 17 models available, Claude and I built an orchestrator to route tasks to the best model:</p>
<div class="highlight">
  <pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code># Designed collaboratively with Claude Code
class ModelSupervisor:
    def __init__(self):
        self.models = {
            &amp;#34;reasoning&amp;#34;: &amp;#34;deepseek-r1:7b&amp;#34;,
            &amp;#34;general&amp;#34;: &amp;#34;mistral:7b-instruct&amp;#34;,
            &amp;#34;code&amp;#34;: &amp;#34;codellama:7b&amp;#34;,
            &amp;#34;fast&amp;#34;: &amp;#34;phi-3:mini&amp;#34;,
            &amp;#34;creative&amp;#34;: &amp;#34;nous-hermes-2:latest&amp;#34;,
            &amp;#34;long_context&amp;#34;: &amp;#34;llama3.1:8b&amp;#34;  # 128K context!
        }

    def route_task(self, task_type: str, prompt: str) -&amp;gt; str:
        &amp;#34;&amp;#34;&amp;#34;Route task to optimal model.&amp;#34;&amp;#34;&amp;#34;
        model = self.models.get(task_type, self.models[&amp;#34;general&amp;#34;])

        response = requests.post(
            &amp;#34;http://localhost:11434/api/generate&amp;#34;,
            json={&amp;#34;model&amp;#34;: model, &amp;#34;prompt&amp;#34;: prompt}
        )

        return response.json()[&amp;#34;response&amp;#34;]</code></pre>
</div>
<div class="highlight">
  <pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code>┌────────────────────────────────────────────────┐
│   Supervisor Pattern Routing (Sept 19, AM)    │
│      (Designed with Claude Code&amp;#39;s help)        │
└────────────────────────────────────────────────┘

                 ┌─────────────────┐
                 │   Supervisor    │
                 │    Decides      │
                 └────────┬────────┘
                          │
         ┌────────────────┼────────────────┐
         │                │                │
         ▼                ▼                ▼
  ┌──────────┐     ┌──────────┐    ┌──────────┐
  │ DeepSeek │     │ CodeLlama│    │  Llama   │
  │    R1    │     │    7B    │    │  3.1 8B  │
  └──────────┘     └──────────┘    └──────────┘
  Reasoning        Code Gen        Long Context
  32K tokens       16K tokens      128K tokens

ROUTING LOGIC:
• Code review    → CodeLlama (specialized)
• Long analysis  → Llama 3.1 (128K context)
• Deep reasoning → DeepSeek R1 (quality)
• Quick answers  → Phi-3 Mini (speed)</code></pre>
</div>
<p>The system could now <strong>self-optimize</strong> based on context needs.</p>
<h2 id="the-reality-a-weekend-project-while-working-full-time">The Reality: A Weekend Project While Working Full-Time<a hidden class="anchor" aria-hidden="true" href="#the-reality-a-weekend-project-while-working-full-time">#</a></h2>
<p><em>Vault Evidence: Sept 18 journal shows continuous activity from 6:30 AM through 11:00 PM - a full weekend day of focused work.</em></p>
<p><strong>This wasn&rsquo;t a quick evening project.</strong> The Sept 18 reflection journal shows:</p>
<ul>
<li>Morning (6:30 AM): Starting automation systems</li>
<li>Afternoon: Ollama installation and model collection</li>
<li>Evening (through 11:00 PM): Supervisor pattern, testing, integration</li>
</ul>
<p><strong>But it was also broken up by life</strong>:</p>
<ul>
<li>Work at BT during the week (Monday-Friday)</li>
<li>Saturday-Sunday: Personal project time</li>
<li>Breaks between intense coding sessions</li>
<li>Real life happening around the development</li>
</ul>
<p><strong>The journal shows the reality</strong>: This was focused weekend work, not a corporate &ldquo;sprint&rdquo;. Personal time, personal pace, personal project.</p>
<h2 id="what-worked">What Worked<a hidden class="anchor" aria-hidden="true" href="#what-worked">#</a></h2>
<p><strong>Working with Claude Code</strong>: This supervisor pattern, model selection strategy, VRAM optimization - all designed collaboratively. Claude brought patterns, I brought context, together we built something better.</p>
<p><strong>Ollama&rsquo;s Model Management</strong>:
Single command to pull, update, or remove models. No Docker containers, no config files, no complexity.</p>
<p><strong>Context Persistence</strong>:
Finally solved the original Day Zero problem - no more losing conversation history!</p>
<p><strong>GPU Performance</strong>:
RTX 4080 handled everything I threw at it. 16GB VRAM was the sweet spot for running multiple 7B models.</p>
<p><strong>Privacy &amp; Control</strong>:
All conversations stay local. No external logging. Complete ownership of my AI interactions.</p>
<h2 id="what-still-sucked">What Still Sucked<a hidden class="anchor" aria-hidden="true" href="#what-still-sucked">#</a></h2>
<p><strong>Model Switching Latency</strong>:
Loading a new model: 2-4 seconds. Not terrible, but noticeable when switching frequently.</p>
<p><strong>VRAM Juggling</strong>:
Can&rsquo;t run Mixtral 8x7B (14.2GB) alongside anything else. Had to be strategic about which models stayed loaded.</p>
<p><strong>Quality Variance</strong>:
Some models (Phi-3 Mini) were fast but shallow. Others (DeepSeek R1) were brilliant but slower. Required testing to find the right fit.</p>
<p><strong>Still Need Claude Code</strong>:
Local models are good, but Claude Code&rsquo;s reasoning is still unmatched for complex tasks. Ollama complements, doesn&rsquo;t replace.</p>
<h2 id="the-numbers-sept-18-19-2025">The Numbers (Sept 18-19, 2025)<a hidden class="anchor" aria-hidden="true" href="#the-numbers-sept-18-19-2025">#</a></h2>
<table>
  <thead>
      <tr>
          <th>Metric</th>
          <th>Value</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td><strong>Time Spent</strong></td>
          <td>Weekend (Saturday-Sunday)</td>
      </tr>
      <tr>
          <td><strong>Work Hours</strong></td>
          <td>~15 hours (split across 2 days)</td>
      </tr>
      <tr>
          <td><strong>Models Installed</strong></td>
          <td>17</td>
      </tr>
      <tr>
          <td><strong>Total Download Size</strong></td>
          <td>78GB</td>
      </tr>
      <tr>
          <td><strong>VRAM Available</strong></td>
          <td>16GB (RTX 4080)</td>
      </tr>
      <tr>
          <td><strong>Context Limit Freedom</strong></td>
          <td>Unlimited (hardware-bound)</td>
      </tr>
      <tr>
          <td><strong>Average Response Time</strong></td>
          <td>2.1 seconds</td>
      </tr>
      <tr>
          <td><strong>Concurrent Models</strong></td>
          <td>3 (12.4GB VRAM)</td>
      </tr>
      <tr>
          <td><strong>External Dependencies</strong></td>
          <td>Eliminated</td>
      </tr>
  </tbody>
</table>
<p><code>★ Insight ─────────────────────────────────────</code>
<strong>The Freedom of Local Inference:</strong></p>
<p>Switching to local LLMs wasn&rsquo;t about cost - it was about <strong>solving the original problem</strong>:</p>
<ol>
<li><strong>Ownership</strong> - You control when conversations end, not a service</li>
<li><strong>Privacy</strong> - Conversations never leave the machine</li>
<li><strong>Offline capability</strong> - No internet required</li>
<li><strong>Experimentation freedom</strong> - Iterate without external throttling</li>
<li><strong>Learning</strong> - Direct access to model internals, VRAM, performance tuning</li>
<li><strong>Choice</strong> - Pick models with context windows matching your needs</li>
</ol>
<p><strong>This was built working WITH Claude Code</strong> - collaborative AI development where human understanding + AI patterns created better solutions than either alone.</p>
<p>The cost savings ($0/year vs potential API costs) were a bonus. The real win was <strong>control over the context window</strong>.</p>
<p><strong>Day Zero&rsquo;s context window problem? Not eliminated - but now under MY control.</strong>
<code>─────────────────────────────────────────────────</code></p>
<h2 id="what-i-learned">What I Learned<a hidden class="anchor" aria-hidden="true" href="#what-i-learned">#</a></h2>
<p><strong>1. Weekend projects fit around full-time work</strong>
Saturday-Sunday intensive work. Monday-Friday back to day job. This is the reality of personal projects.</p>
<p><strong>2. Collaboration makes better solutions</strong>
Claude Code + my domain knowledge = supervisor pattern we wouldn&rsquo;t have designed individually.</p>
<p><strong>3. Control over context &gt; raw performance</strong>
Having the option to manage conversation memory yourself is more valuable than slightly faster responses from a service that forces compaction.</p>
<p><strong>4. Privacy enables experimentation</strong>
Knowing conversations stay local removes psychological barriers to trying wild ideas.</p>
<p><strong>5. Local doesn&rsquo;t mean isolated</strong>
Ollama + Claude Code = best of both worlds. Use local for persistent work, cloud for complex reasoning.</p>
<h2 id="whats-next">What&rsquo;s Next<a hidden class="anchor" aria-hidden="true" href="#whats-next">#</a></h2>
<p>Ollama was running. I had local control over my AI conversations. But the system was generating responses faster than I could organize them.</p>
<p>Working with Ollama over the next few days would generate hundreds more conversation files. By September 20, I&rsquo;d need a way to search them all.</p>
<p>That&rsquo;s when ChromaDB and semantic search would enter the picture.</p>
<hr>
<p><em>This is Episode 3 of &ldquo;Season 1: From Zero to Automated Infrastructure&rdquo; - documenting the weekend that solved the context window problem with local AI.</em></p>
<p><em>Previous Episode</em>: <a href="season-1-episode-2-mvp">Building the Foundation: Vault Creation to MVP</a>
<em>Next Episode</em>: <a href="season-1-episode-4-documentation-overload">ChromaDB Weekend: From 504 to 24,916 Documents</a>
<em>Complete Series</em>: <a href="/tags/season-1/">Season 1 Mapping Report</a></p>
<hr>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
      <li><a href="https://blog.rduffy.uk/tags/ollama/">Ollama</a></li>
      <li><a href="https://blog.rduffy.uk/tags/local-llm/">Local-Llm</a></li>
      <li><a href="https://blog.rduffy.uk/tags/deepseek/">Deepseek</a></li>
      <li><a href="https://blog.rduffy.uk/tags/rtx-4080/">Rtx-4080</a></li>
      <li><a href="https://blog.rduffy.uk/tags/ai-infrastructure/">Ai-Infrastructure</a></li>
      <li><a href="https://blog.rduffy.uk/tags/collaboration/">Collaboration</a></li>
      <li><a href="https://blog.rduffy.uk/tags/context-windows/">Context-Windows</a></li>
    </ul>
<nav class="paginav">
  <a class="prev" href="https://blog.rduffy.uk/posts/season-1-episode-7-diagram-automation/">
    <span class="title">« Prev</span>
    <br>
    <span>Teaching the System to Document Itself: Automated Architecture Diagrams</span>
  </a>
  <a class="next" href="https://blog.rduffy.uk/posts/season-1-episode-5-migration-question/">
    <span class="title">Next »</span>
    <br>
    <span>The Migration Question: When K3s Meets Reality</span>
  </a>
</nav>


<ul class="share-buttons">
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share The AI Awakening: Breaking Free from Context Limits on x"
            href="https://x.com/intent/tweet/?text=The%20AI%20Awakening%3a%20Breaking%20Free%20from%20Context%20Limits&amp;url=https%3a%2f%2fblog.rduffy.uk%2fposts%2fseason-1-episode-3-ai-awakening%2f&amp;hashtags=ollama%2clocal-llm%2cdeepseek%2crtx-4080%2cai-infrastructure%2ccollaboration%2ccontext-windows">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M512 62.554 L 512 449.446 C 512 483.97 483.97 512 449.446 512 L 62.554 512 C 28.03 512 0 483.97 0 449.446 L 0 62.554 C 0 28.03 28.029 0 62.554 0 L 449.446 0 C 483.971 0 512 28.03 512 62.554 Z M 269.951 190.75 L 182.567 75.216 L 56 75.216 L 207.216 272.95 L 63.9 436.783 L 125.266 436.783 L 235.9 310.383 L 332.567 436.783 L 456 436.783 L 298.367 228.367 L 432.367 75.216 L 371.033 75.216 Z M 127.633 110 L 164.101 110 L 383.481 400.065 L 349.5 400.065 Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share The AI Awakening: Breaking Free from Context Limits on linkedin"
            href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2fblog.rduffy.uk%2fposts%2fseason-1-episode-3-ai-awakening%2f&amp;title=The%20AI%20Awakening%3a%20Breaking%20Free%20from%20Context%20Limits&amp;summary=The%20AI%20Awakening%3a%20Breaking%20Free%20from%20Context%20Limits&amp;source=https%3a%2f%2fblog.rduffy.uk%2fposts%2fseason-1-episode-3-ai-awakening%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-288.985,423.278l0,-225.717l-75.04,0l0,225.717l75.04,0Zm270.539,0l0,-129.439c0,-69.333 -37.018,-101.586 -86.381,-101.586c-39.804,0 -57.634,21.891 -67.617,37.266l0,-31.958l-75.021,0c0.995,21.181 0,225.717 0,225.717l75.02,0l0,-126.056c0,-6.748 0.486,-13.492 2.474,-18.315c5.414,-13.475 17.767,-27.434 38.494,-27.434c27.135,0 38.007,20.707 38.007,51.037l0,120.768l75.024,0Zm-307.552,-334.556c-25.674,0 -42.448,16.879 -42.448,39.002c0,21.658 16.264,39.002 41.455,39.002l0.484,0c26.165,0 42.452,-17.344 42.452,-39.002c-0.485,-22.092 -16.241,-38.954 -41.943,-39.002Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share The AI Awakening: Breaking Free from Context Limits on reddit"
            href="https://reddit.com/submit?url=https%3a%2f%2fblog.rduffy.uk%2fposts%2fseason-1-episode-3-ai-awakening%2f&title=The%20AI%20Awakening%3a%20Breaking%20Free%20from%20Context%20Limits">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-3.446,265.638c0,-22.964 -18.616,-41.58 -41.58,-41.58c-11.211,0 -21.361,4.457 -28.841,11.666c-28.424,-20.508 -67.586,-33.757 -111.204,-35.278l18.941,-89.121l61.884,13.157c0.756,15.734 13.642,28.29 29.56,28.29c16.407,0 29.706,-13.299 29.706,-29.701c0,-16.403 -13.299,-29.702 -29.706,-29.702c-11.666,0 -21.657,6.792 -26.515,16.578l-69.105,-14.69c-1.922,-0.418 -3.939,-0.042 -5.585,1.036c-1.658,1.073 -2.811,2.761 -3.224,4.686l-21.152,99.438c-44.258,1.228 -84.046,14.494 -112.837,35.232c-7.468,-7.164 -17.589,-11.591 -28.757,-11.591c-22.965,0 -41.585,18.616 -41.585,41.58c0,16.896 10.095,31.41 24.568,37.918c-0.639,4.135 -0.99,8.328 -0.99,12.576c0,63.977 74.469,115.836 166.33,115.836c91.861,0 166.334,-51.859 166.334,-115.836c0,-4.218 -0.347,-8.387 -0.977,-12.493c14.564,-6.47 24.735,-21.034 24.735,-38.001Zm-119.474,108.193c-20.27,20.241 -59.115,21.816 -70.534,21.816c-11.428,0 -50.277,-1.575 -70.522,-21.82c-3.007,-3.008 -3.007,-7.882 0,-10.889c3.003,-2.999 7.882,-3.003 10.885,0c12.777,12.781 40.11,17.317 59.637,17.317c19.522,0 46.86,-4.536 59.657,-17.321c3.016,-2.999 7.886,-2.995 10.885,0.008c3.008,3.011 3.003,7.882 -0.008,10.889Zm-5.23,-48.781c-16.373,0 -29.701,-13.324 -29.701,-29.698c0,-16.381 13.328,-29.714 29.701,-29.714c16.378,0 29.706,13.333 29.706,29.714c0,16.374 -13.328,29.698 -29.706,29.698Zm-160.386,-29.702c0,-16.381 13.328,-29.71 29.714,-29.71c16.369,0 29.689,13.329 29.689,29.71c0,16.373 -13.32,29.693 -29.689,29.693c-16.386,0 -29.714,-13.32 -29.714,-29.693Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share The AI Awakening: Breaking Free from Context Limits on facebook"
            href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fblog.rduffy.uk%2fposts%2fseason-1-episode-3-ai-awakening%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-106.468,0l0,-192.915l66.6,0l12.672,-82.621l-79.272,0l0,-53.617c0,-22.603 11.073,-44.636 46.58,-44.636l36.042,0l0,-70.34c0,0 -32.71,-5.582 -63.982,-5.582c-65.288,0 -107.96,39.569 -107.96,111.204l0,62.971l-72.573,0l0,82.621l72.573,0l0,192.915l-191.104,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share The AI Awakening: Breaking Free from Context Limits on whatsapp"
            href="https://api.whatsapp.com/send?text=The%20AI%20Awakening%3a%20Breaking%20Free%20from%20Context%20Limits%20-%20https%3a%2f%2fblog.rduffy.uk%2fposts%2fseason-1-episode-3-ai-awakening%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-58.673,127.703c-33.842,-33.881 -78.847,-52.548 -126.798,-52.568c-98.799,0 -179.21,80.405 -179.249,179.234c-0.013,31.593 8.241,62.428 23.927,89.612l-25.429,92.884l95.021,-24.925c26.181,14.28 55.659,21.807 85.658,21.816l0.074,0c98.789,0 179.206,-80.413 179.247,-179.243c0.018,-47.895 -18.61,-92.93 -52.451,-126.81Zm-126.797,275.782l-0.06,0c-26.734,-0.01 -52.954,-7.193 -75.828,-20.767l-5.441,-3.229l-56.386,14.792l15.05,-54.977l-3.542,-5.637c-14.913,-23.72 -22.791,-51.136 -22.779,-79.287c0.033,-82.142 66.867,-148.971 149.046,-148.971c39.793,0.014 77.199,15.531 105.329,43.692c28.128,28.16 43.609,65.592 43.594,105.4c-0.034,82.149 -66.866,148.983 -148.983,148.984Zm81.721,-111.581c-4.479,-2.242 -26.499,-13.075 -30.604,-14.571c-4.105,-1.495 -7.091,-2.241 -10.077,2.241c-2.986,4.483 -11.569,14.572 -14.182,17.562c-2.612,2.988 -5.225,3.364 -9.703,1.12c-4.479,-2.241 -18.91,-6.97 -36.017,-22.23c-13.314,-11.876 -22.304,-26.542 -24.916,-31.026c-2.612,-4.484 -0.279,-6.908 1.963,-9.14c2.016,-2.007 4.48,-5.232 6.719,-7.847c2.24,-2.615 2.986,-4.484 4.479,-7.472c1.493,-2.99 0.747,-5.604 -0.374,-7.846c-1.119,-2.241 -10.077,-24.288 -13.809,-33.256c-3.635,-8.733 -7.327,-7.55 -10.077,-7.688c-2.609,-0.13 -5.598,-0.158 -8.583,-0.158c-2.986,0 -7.839,1.121 -11.944,5.604c-4.105,4.484 -15.675,15.32 -15.675,37.364c0,22.046 16.048,43.342 18.287,46.332c2.24,2.99 31.582,48.227 76.511,67.627c10.685,4.615 19.028,7.371 25.533,9.434c10.728,3.41 20.492,2.929 28.209,1.775c8.605,-1.285 26.499,-10.833 30.231,-21.295c3.732,-10.464 3.732,-19.431 2.612,-21.298c-1.119,-1.869 -4.105,-2.99 -8.583,-5.232Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share The AI Awakening: Breaking Free from Context Limits on telegram"
            href="https://telegram.me/share/url?text=The%20AI%20Awakening%3a%20Breaking%20Free%20from%20Context%20Limits&amp;url=https%3a%2f%2fblog.rduffy.uk%2fposts%2fseason-1-episode-3-ai-awakening%2f">
            <svg version="1.1" xml:space="preserve" viewBox="2 2 28 28" height="30px" width="30px" fill="currentColor">
                <path
                    d="M26.49,29.86H5.5a3.37,3.37,0,0,1-2.47-1,3.35,3.35,0,0,1-1-2.47V5.48A3.36,3.36,0,0,1,3,3,3.37,3.37,0,0,1,5.5,2h21A3.38,3.38,0,0,1,29,3a3.36,3.36,0,0,1,1,2.46V26.37a3.35,3.35,0,0,1-1,2.47A3.38,3.38,0,0,1,26.49,29.86Zm-5.38-6.71a.79.79,0,0,0,.85-.66L24.73,9.24a.55.55,0,0,0-.18-.46.62.62,0,0,0-.41-.17q-.08,0-16.53,6.11a.59.59,0,0,0-.41.59.57.57,0,0,0,.43.52l4,1.24,1.61,4.83a.62.62,0,0,0,.63.43.56.56,0,0,0,.4-.17L16.54,20l4.09,3A.9.9,0,0,0,21.11,23.15ZM13.8,20.71l-1.21-4q8.72-5.55,8.78-5.55c.15,0,.23,0,.23.16a.18.18,0,0,1,0,.06s-2.51,2.3-7.52,6.8Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share The AI Awakening: Breaking Free from Context Limits on ycombinator"
            href="https://news.ycombinator.com/submitlink?t=The%20AI%20Awakening%3a%20Breaking%20Free%20from%20Context%20Limits&u=https%3a%2f%2fblog.rduffy.uk%2fposts%2fseason-1-episode-3-ai-awakening%2f">
            <svg version="1.1" xml:space="preserve" width="30px" height="30px" viewBox="0 0 512 512" fill="currentColor"
                xmlns:inkscape="http://www.inkscape.org/namespaces/inkscape">
                <path
                    d="M449.446 0C483.971 0 512 28.03 512 62.554L512 449.446C512 483.97 483.97 512 449.446 512L62.554 512C28.03 512 0 483.97 0 449.446L0 62.554C0 28.03 28.029 0 62.554 0L449.446 0ZM183.8767 87.9921H121.8427L230.6673 292.4508V424.0079H281.3328V292.4508L390.1575 87.9921H328.1233L256 238.2489z" />
            </svg>
        </a>
    </li>
</ul>

  </footer>
</article>
    </main>
    
<footer class="footer">
        <span>&copy; 2025 <a href="https://blog.rduffy.uk/">Ryan Duffy - AI Infrastructure &amp; Local LLM Journey</a></span> · 

    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
<script>
    document.querySelectorAll('pre > code').forEach((codeblock) => {
        const container = codeblock.parentNode.parentNode;

        const copybutton = document.createElement('button');
        copybutton.classList.add('copy-code');
        copybutton.innerHTML = 'copy';

        function copyingDone() {
            copybutton.innerHTML = 'copied!';
            setTimeout(() => {
                copybutton.innerHTML = 'copy';
            }, 2000);
        }

        copybutton.addEventListener('click', (cb) => {
            if ('clipboard' in navigator) {
                navigator.clipboard.writeText(codeblock.textContent);
                copyingDone();
                return;
            }

            const range = document.createRange();
            range.selectNodeContents(codeblock);
            const selection = window.getSelection();
            selection.removeAllRanges();
            selection.addRange(range);
            try {
                document.execCommand('copy');
                copyingDone();
            } catch (e) { };
            selection.removeRange(range);
        });

        if (container.classList.contains("highlight")) {
            container.appendChild(copybutton);
        } else if (container.parentNode.firstChild == container) {
            
        } else if (codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName == "TABLE") {
            
            codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(copybutton);
        } else {
            
            codeblock.parentNode.appendChild(copybutton);
        }
    });
</script>
</body>

</html>
