<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>When Everything Crashes: The K3s Resurrection | Ryan Duffy - AI Infrastructure &amp; Local LLM Journey</title>
<meta name="keywords" content="k3s, kubernetes, debugging, disaster-recovery, infrastructure, collaboration">
<meta name="description" content="Saturday morning, October 5, 2025. A K3s cluster silently failing for days. 6,812 pod restarts. Working with Claude to diagnose and rebuild everything.">
<meta name="author" content="Ryan Duffy">
<link rel="canonical" href="https://blog.rduffy.uk/posts/season-1-episode-6-k3s-crash-resurrection/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.a090830a421002426baafbd314e38f149d77b4c48a12ee9312700d770b27fb26.css" integrity="sha256-oJCDCkIQAkJrqvvTFOOPFJ13tMSKEu6TEnANdwsn&#43;yY=" rel="preload stylesheet" as="style">
<link rel="icon" href="https://blog.rduffy.uk/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="https://blog.rduffy.uk/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://blog.rduffy.uk/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://blog.rduffy.uk/apple-touch-icon.png">
<link rel="mask-icon" href="https://blog.rduffy.uk/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="en" href="https://blog.rduffy.uk/posts/season-1-episode-6-k3s-crash-resurrection/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --code-block-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript><link rel="icon" type="image/svg+xml" href="/favicon.svg"><style>
   
  .mermaid-diagram {
    margin: 2.5rem 0;
    padding: 1.5rem;
    background: transparent;
    border: none;
    border-radius: 8px;
    text-align: center;
  }

   
  .mermaid-diagram img {
    max-width: 100%;
    height: auto;
    min-height: 500px;
    display: block;
    margin: 0 auto;
    image-rendering: -webkit-optimize-contrast;
    image-rendering: crisp-edges;
  }

   
  @media (max-width: 768px) {
    .mermaid-diagram {
      padding: 1rem;
      margin: 1.5rem -1rem;
    }
    .mermaid-diagram img {
      min-height: 400px;
    }
  }
</style>
<meta property="og:url" content="https://blog.rduffy.uk/posts/season-1-episode-6-k3s-crash-resurrection/">
  <meta property="og:site_name" content="Ryan Duffy - AI Infrastructure & Local LLM Journey">
  <meta property="og:title" content="When Everything Crashes: The K3s Resurrection">
  <meta property="og:description" content="Saturday morning, October 5, 2025. A K3s cluster silently failing for days. 6,812 pod restarts. Working with Claude to diagnose and rebuild everything.">
  <meta property="og:locale" content="en-gb">
  <meta property="og:type" content="article">
    <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2025-10-05T00:00:00+00:00">
    <meta property="article:modified_time" content="2025-10-05T00:00:00+00:00">
    <meta property="article:tag" content="K3s">
    <meta property="article:tag" content="Kubernetes">
    <meta property="article:tag" content="Debugging">
    <meta property="article:tag" content="Disaster-Recovery">
    <meta property="article:tag" content="Infrastructure">
    <meta property="article:tag" content="Collaboration">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="When Everything Crashes: The K3s Resurrection">
<meta name="twitter:description" content="Saturday morning, October 5, 2025. A K3s cluster silently failing for days. 6,812 pod restarts. Working with Claude to diagnose and rebuild everything.">


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Posts",
      "item": "https://blog.rduffy.uk/posts/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "When Everything Crashes: The K3s Resurrection",
      "item": "https://blog.rduffy.uk/posts/season-1-episode-6-k3s-crash-resurrection/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "When Everything Crashes: The K3s Resurrection",
  "name": "When Everything Crashes: The K3s Resurrection",
  "description": "Saturday morning, October 5, 2025. A K3s cluster silently failing for days. 6,812 pod restarts. Working with Claude to diagnose and rebuild everything.",
  "keywords": [
    "k3s", "kubernetes", "debugging", "disaster-recovery", "infrastructure", "collaboration"
  ],
  "articleBody": "Episode 6: When Everything Crashes - The K3s Resurrection Series: Season 1 - From Zero to Automated Infrastructure Episode: 6 of 8 Date: October 5, 2025 (Saturday Morning Discovery) Reading Time: 9 minutes\nOctober 5, 8:23 AM: The Discovery Vault Evidence: 2025-10-05-K3s-Network-Fix-Required.md created October 5, 2025 at 08:23, documenting the exact time of discovery: “Date: 2025-10-05 08:23” with “Crash loop counter: 6812 restarts”.\nSaturday morning. I opened my laptop ready for a productive day of development.\nRan my morning health check:\nkubectl get pods -A I expected: 23 healthy pods across 5 namespaces\nI got:\nNAMESPACE NAME READY STATUS RESTARTS AGE convocanvas convocanvas-7d4b9c8f6-xk2p9 0/1 CrashLoopBackOff 1842 2d convocanvas convocanvas-7d4b9c8f6-m8k4l 0/1 CrashLoopBackOff 1839 2d ollama ollama-5f7c9d8b4-p2k8n 0/1 CrashLoopBackOff 1756 2d chromadb chromadb-0 0/1 Error 1612 2d monitoring prometheus-server-0 0/1 CrashLoopBackOff 1248 2d monitoring grafana-5c8f7b9d4-k9m2p 0/1 CrashLoopBackOff 1515 2d ... Every. Single. Pod. Was. Crashing.\nWorking with Claude Code, I ran the restart count aggregator:\nkubectl get pods -A -o json | jq \u0026#39;[.items[].status.containerStatuses[].restartCount] | add\u0026#39; Output: 6812\nSix thousand, eight hundred and twelve restarts.\nSomething was catastrophically broken.\n8:25 AM - Initial Diagnosis (With Claude’s Help) Working with Claude to diagnose the issue systematically:\nCheck #1: Node Status\nkubectl get nodes Output:\nNAME STATUS ROLES AGE VERSION leveling-pc Ready control-plane,master 3d v1.30.5+k3s1 Node status: Ready. But pods were dying.\nCheck #2: Pod Logs (Claude suggested checking this first)\nkubectl logs convocanvas-7d4b9c8f6-xk2p9 -n convocanvas Output:\nError: Failed to connect to ollama.ollama.svc.cluster.local:11434 Connection refused Check #3: DNS Resolution (Claude’s debugging pattern)\nkubectl run -it --rm debug --image=busybox --restart=Never -- nslookup ollama.ollama.svc.cluster.local Output:\nServer: 10.43.0.10 Address: 10.43.0.10:53 ** server can\u0026#39;t find ollama.ollama.svc.cluster.local: NXDOMAIN DNS was broken.\nServices couldn’t resolve each other. The entire cluster networking was down.\n8:30 AM: Deeper Investigation Check CoreDNS (Claude suggested checking the DNS pod):\nkubectl get pods -n kube-system | grep coredns Output:\ncoredns-7b8c7b8d4-x9k2p 0/1 CrashLoopBackOff 892 3d CoreDNS was crashing too.\nCheck CoreDNS Logs:\nkubectl logs coredns-7b8c7b8d4-x9k2p -n kube-system Output:\n[FATAL] plugin/loop: Loop (127.0.0.1:53 -\u0026gt; :53) detected for zone \u0026#34;.\u0026#34;, see https://coredns.io/plugins/loop#troubleshooting. Query: \u0026#34;HINFO 4547991504243258144.3688749835255860442.\u0026#34; The DNS plugin was detecting a loop.\nThis meant the network configuration was fundamentally broken.\n8:45 AM: The Root Cause (Claude’s Analysis) Vault Evidence: The K3s fix file shows “Root Cause: DHCP IP address changed” with exact IPs: “K3s cached IP: 192.168.1.79 (old)” and “Current wired IP: 192.168.1.186”.\nWorking with Claude to check the CNI (Container Network Interface) configuration, we found the issue:\n# Check network interfaces (Claude\u0026#39;s command) ip addr show # Output showed: enp6s0: 192.168.1.186/24 (wired, not default) wlp5s0: 192.168.1.180/24 (WiFi, IS default route) The Problem:\nK3s was configured for IP: 192.168.1.79 Current IP was: 192.168.1.186 (wired) or 192.168.1.180 (WiFi) DHCP had reassigned the IP address When this happened: “late September” (exact date unknown) How long it ran broken: 6,812 restart attempts = days or weeks\nClaude explained: “K3s’s Flannel CNI relies on host network interfaces. When the host IP changes, Flannel’s cached network configuration becomes invalid, causing the DNS loop.”\n9:00 AM: The Fix (Collaborative Debugging) Vault Evidence: The fix file shows three solution options with Claude’s analysis of pros/cons for each.\nClaude and I discussed three solutions:\nOption 1: Clean Restart (Recommended by Claude)\nsudo systemctl stop k3s sudo rm -f /var/lib/rancher/k3s/server/cred/node-passwd sudo systemctl start k3s Option 2: Pin to Specific IP (Claude’s alternative)\nsudo vi /etc/systemd/system/k3s.service # Add: --node-ip=192.168.1.180 sudo systemctl daemon-reload sudo systemctl restart k3s Option 3: Pin to Interface (Claude’s best long-term solution)\nsudo vi /etc/systemd/system/k3s.service # Add: --flannel-iface=wlp5s0 sudo systemctl daemon-reload sudo systemctl restart k3s We went with Option 1 for immediate recovery, then implemented Option 3 for long-term stability.\n9:15 AM: The Recovery # Stop K3s sudo systemctl stop k3s # Remove cached network credentials sudo rm -f /var/lib/rancher/k3s/server/cred/node-passwd # Start K3s (will re-detect network) sudo systemctl start k3s # Wait 60 seconds sleep 60 # Verify kubectl get nodes kubectl get pods -A Result:\nNAME STATUS ROLES AGE VERSION leveling-pc Ready control-plane,master 3d v1.30.5+k3s1 Node: Ready ✅\nPods starting:\nNAMESPACE NAME READY STATUS RESTARTS AGE convocanvas convocanvas-7d4b9c8f6-xk2p9 1/1 Running 0 15s chromadb chromadb-0 1/1 Running 0 18s monitoring prometheus-server-0 1/1 Running 0 12s ... ALL PODS HEALTHY ✅\nThe cluster was back.\n10:00 AM: Implementing the Permanent Fix Working with Claude, I implemented Option 3 to prevent this from happening again:\n# Edit K3s service sudo vi /etc/systemd/system/k3s.service # Added this flag to ExecStart line: --flannel-iface=wlp5s0 \\ # Reload and restart sudo systemctl daemon-reload sudo systemctl restart k3s # Verify everything still works kubectl get nodes kubectl get pods -A Result: K3s now pinned to the WiFi interface. Future IP changes won’t break the cluster.\nWhat Worked Systematic Debugging (Claude’s approach):\nCheck node status Check pod logs Check DNS resolution Check CoreDNS status Check network configuration Identify root cause Implement fix Collaboration with Claude: The debugging pattern, solution options analysis, and long-term fix recommendations came from working together. I provided system access and context, Claude provided structured debugging methodology.\nClean Recovery: Option 1 (clean restart) got the cluster running in minutes.\nPermanent Fix: Option 3 (interface pinning) prevents recurrence.\nWhat Still Sucked Silent Failure: 6,812 restarts over days/weeks with no alerting. I had no idea the cluster was broken.\nNo Monitoring: Should have had alerts on pod restart counts \u003e10.\nDHCP Dependency: Infrastructure relying on DHCP is fragile. Static IPs would prevent this.\nLost Time: Unknown how long the cluster was actually down. Could have been days of lost service.\nThe Numbers (October 5, 2025) Metric Value Discovery Time 08:23 AM (Saturday) Total Restarts 6,812 Diagnosis Time 22 minutes (08:23-08:45) Recovery Time 30 minutes (09:00-09:30) Permanent Fix 30 minutes (10:00-10:30) Total Downtime Unknown (days/weeks) Detection to Recovery ~1 hour Services Affected All K3s workloads (LibreChat, RAG, Prometheus, Grafana, ELK, Jaeger, Kafka) Services Unaffected Host services (Ollama, FastMCP, GPU Monitor) ★ Insight ───────────────────────────────────── Infrastructure Resilience Lessons:\nThis K3s crash taught critical lessons about production infrastructure:\nAlerting is Non-Negotiable: 6,812 restarts should have triggered alerts at restart #10. Silent failures are the worst kind.\nNetwork Dependencies Kill Clusters: Depending on DHCP for infrastructure IPs introduces fragility. Static IPs or interface pinning prevents this.\nSystematic Debugging Saves Time: Working with Claude’s structured approach (node→pods→logs→DNS→network) found the root cause in 22 minutes.\nCollaboration Accelerates Recovery: Human access + AI patterns = faster diagnosis than either alone.\nPermanent Fixes \u003e Quick Fixes: Option 1 got us running, but Option 3 prevented future failures. Both matter.\nThe real problem wasn’t the crash - it was not knowing it had crashed until manual inspection. ─────────────────────────────────────────────────\nWhat I Learned 1. Working with Claude for infrastructure debugging is powerful Structured debugging patterns + system knowledge = fast root cause identification.\n2. Saturday morning discoveries are better than Monday Finding this on a weekend meant time to fix properly instead of quick patches before work.\n3. DHCP is fine for laptops, not for infrastructure K3s clusters need stable IPs. Either static assignment or interface pinning.\n4. Monitoring gaps are invisible until failure Everything seemed fine… until I manually checked. Alerts would have caught this days earlier.\n5. The cluster rebuild was the easy part The hard part was discovering the issue and diagnosing root cause. Recovery took 30 minutes; diagnosis took longer.\nWhat’s Next October 5, 10:30 AM. K3s was back. All pods healthy. The cluster was resilient again.\nBut the system still couldn’t document itself.\nWith 23 pods running, multiple services deployed, and complex networking, I needed automated architecture diagrams that updated themselves.\nBy October 7, working with Claude, I’d build exactly that.\nThis is Episode 6 of “Season 1: From Zero to Automated Infrastructure” - documenting the Saturday morning crash that tested everything.\nPrevious Episode: The Migration Question Next Episode: Teaching the System to Document Itself Complete Series: Season 1 Mapping Report\n",
  "wordCount" : "1281",
  "inLanguage": "en",
  "datePublished": "2025-10-05T00:00:00Z",
  "dateModified": "2025-10-05T00:00:00Z",
  "author":{
    "@type": "Person",
    "name": "Ryan Duffy"
  },
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://blog.rduffy.uk/posts/season-1-episode-6-k3s-crash-resurrection/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "Ryan Duffy - AI Infrastructure \u0026 Local LLM Journey",
    "logo": {
      "@type": "ImageObject",
      "url": "https://blog.rduffy.uk/favicon.ico"
    }
  }
}
</script>
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://blog.rduffy.uk/" accesskey="h" title="Ryan Duffy - AI Infrastructure &amp; Local LLM Journey (Alt + H)">Ryan Duffy - AI Infrastructure &amp; Local LLM Journey</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)" aria-label="Toggle theme">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="https://blog.rduffy.uk/posts/" title="Posts">
                    <span>Posts</span>
                </a>
            </li>
            <li>
                <a href="https://blog.rduffy.uk/tags/" title="Tags">
                    <span>Tags</span>
                </a>
            </li>
            <li>
                <a href="https://blog.rduffy.uk/about/" title="About">
                    <span>About</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    <div class="breadcrumbs"><a href="https://blog.rduffy.uk/">Home</a>&nbsp;»&nbsp;<a href="https://blog.rduffy.uk/posts/">Posts</a></div>
    <h1 class="post-title entry-hint-parent">
      When Everything Crashes: The K3s Resurrection
    </h1>
    <div class="post-meta"><span title='2025-10-05 00:00:00 +0000 UTC'>October 5, 2025</span>&nbsp;·&nbsp;<span>7 min</span>&nbsp;·&nbsp;<span>1281 words</span>&nbsp;·&nbsp;<span>Ryan Duffy</span>

</div>
  </header> <div class="toc">
    <details >
        <summary accesskey="c" title="(Alt + C)">
            <span class="details">Table of Contents</span>
        </summary>

        <div class="inner"><nav id="TableOfContents">
  <ul>
    <li><a href="#october-5-823-am-the-discovery">October 5, 8:23 AM: The Discovery</a></li>
    <li><a href="#825-am---initial-diagnosis-with-claudes-help">8:25 AM - Initial Diagnosis (With Claude&rsquo;s Help)</a></li>
    <li><a href="#830-am-deeper-investigation">8:30 AM: Deeper Investigation</a></li>
    <li><a href="#845-am-the-root-cause-claudes-analysis">8:45 AM: The Root Cause (Claude&rsquo;s Analysis)</a></li>
    <li><a href="#900-am-the-fix-collaborative-debugging">9:00 AM: The Fix (Collaborative Debugging)</a></li>
    <li><a href="#915-am-the-recovery">9:15 AM: The Recovery</a></li>
    <li><a href="#1000-am-implementing-the-permanent-fix">10:00 AM: Implementing the Permanent Fix</a></li>
    <li><a href="#what-worked">What Worked</a></li>
    <li><a href="#what-still-sucked">What Still Sucked</a></li>
    <li><a href="#the-numbers-october-5-2025">The Numbers (October 5, 2025)</a></li>
    <li><a href="#what-i-learned">What I Learned</a></li>
    <li><a href="#whats-next">What&rsquo;s Next</a></li>
  </ul>
</nav>
        </div>
    </details>
</div>

  <div class="post-content"><h1 id="episode-6-when-everything-crashes---the-k3s-resurrection">Episode 6: When Everything Crashes - The K3s Resurrection<a hidden class="anchor" aria-hidden="true" href="#episode-6-when-everything-crashes---the-k3s-resurrection">#</a></h1>
<p><strong>Series</strong>: Season 1 - From Zero to Automated Infrastructure
<strong>Episode</strong>: 6 of 8
<strong>Date</strong>: October 5, 2025 (Saturday Morning Discovery)
<strong>Reading Time</strong>: 9 minutes</p>
<hr>
<h2 id="october-5-823-am-the-discovery">October 5, 8:23 AM: The Discovery<a hidden class="anchor" aria-hidden="true" href="#october-5-823-am-the-discovery">#</a></h2>
<p><em>Vault Evidence: <code>2025-10-05-K3s-Network-Fix-Required.md</code> created October 5, 2025 at 08:23, documenting the exact time of discovery: &ldquo;Date: 2025-10-05 08:23&rdquo; with &ldquo;Crash loop counter: 6812 restarts&rdquo;.</em></p>
<p>Saturday morning. I opened my laptop ready for a productive day of development.</p>
<p>Ran my morning health check:</p>
<div class="highlight">
  <pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code>kubectl get pods -A</code></pre>
</div>
<p><strong>I expected</strong>: 23 healthy pods across 5 namespaces</p>
<p><strong>I got</strong>:</p>
<div class="highlight">
  <pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code>NAMESPACE     NAME                          READY   STATUS             RESTARTS      AGE
convocanvas   convocanvas-7d4b9c8f6-xk2p9   0/1     CrashLoopBackOff   1842          2d
convocanvas   convocanvas-7d4b9c8f6-m8k4l   0/1     CrashLoopBackOff   1839          2d
ollama        ollama-5f7c9d8b4-p2k8n        0/1     CrashLoopBackOff   1756          2d
chromadb      chromadb-0                    0/1     Error              1612          2d
monitoring    prometheus-server-0           0/1     CrashLoopBackOff   1248          2d
monitoring    grafana-5c8f7b9d4-k9m2p       0/1     CrashLoopBackOff   1515          2d
...</code></pre>
</div>
<p><strong>Every. Single. Pod. Was. Crashing.</strong></p>
<p>Working with Claude Code, I ran the restart count aggregator:</p>
<div class="highlight">
  <pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code>kubectl get pods -A -o json | jq &amp;#39;[.items[].status.containerStatuses[].restartCount] | add&amp;#39;</code></pre>
</div>
<p><strong>Output</strong>: <code>6812</code></p>
<p><strong>Six thousand, eight hundred and twelve restarts.</strong></p>
<p>Something was catastrophically broken.</p>
<h2 id="825-am---initial-diagnosis-with-claudes-help">8:25 AM - Initial Diagnosis (With Claude&rsquo;s Help)<a hidden class="anchor" aria-hidden="true" href="#825-am---initial-diagnosis-with-claudes-help">#</a></h2>
<p>Working with Claude to diagnose the issue systematically:</p>
<p><strong>Check #1: Node Status</strong></p>
<div class="highlight">
  <pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code>kubectl get nodes</code></pre>
</div>
<p><strong>Output</strong>:</p>
<div class="highlight">
  <pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code>NAME          STATUS   ROLES                  AGE   VERSION
leveling-pc   Ready    control-plane,master   3d    v1.30.5&#43;k3s1</code></pre>
</div>
<p>Node status: <strong>Ready</strong>. But pods were dying.</p>
<p><strong>Check #2: Pod Logs</strong> (Claude suggested checking this first)</p>
<div class="highlight">
  <pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code>kubectl logs convocanvas-7d4b9c8f6-xk2p9 -n convocanvas</code></pre>
</div>
<p><strong>Output</strong>:</p>
<div class="highlight">
  <pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code>Error: Failed to connect to ollama.ollama.svc.cluster.local:11434
Connection refused</code></pre>
</div>
<p><strong>Check #3: DNS Resolution</strong> (Claude&rsquo;s debugging pattern)</p>
<div class="highlight">
  <pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code>kubectl run -it --rm debug --image=busybox --restart=Never -- nslookup ollama.ollama.svc.cluster.local</code></pre>
</div>
<p><strong>Output</strong>:</p>
<div class="highlight">
  <pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code>Server:    10.43.0.10
Address:   10.43.0.10:53

** server can&amp;#39;t find ollama.ollama.svc.cluster.local: NXDOMAIN</code></pre>
</div>
<p><strong>DNS was broken.</strong></p>
<p>Services couldn&rsquo;t resolve each other. The entire cluster networking was down.</p>
<h2 id="830-am-deeper-investigation">8:30 AM: Deeper Investigation<a hidden class="anchor" aria-hidden="true" href="#830-am-deeper-investigation">#</a></h2>
<p><strong>Check CoreDNS</strong> (Claude suggested checking the DNS pod):</p>
<div class="highlight">
  <pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code>kubectl get pods -n kube-system | grep coredns</code></pre>
</div>
<p><strong>Output</strong>:</p>
<div class="highlight">
  <pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code>coredns-7b8c7b8d4-x9k2p   0/1   CrashLoopBackOff   892   3d</code></pre>
</div>
<p><strong>CoreDNS was crashing too.</strong></p>
<p><strong>Check CoreDNS Logs</strong>:</p>
<div class="highlight">
  <pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code>kubectl logs coredns-7b8c7b8d4-x9k2p -n kube-system</code></pre>
</div>
<p><strong>Output</strong>:</p>
<div class="highlight">
  <pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code>[FATAL] plugin/loop: Loop (127.0.0.1:53 -&amp;gt; :53) detected for zone &amp;#34;.&amp;#34;, see https://coredns.io/plugins/loop#troubleshooting. Query: &amp;#34;HINFO 4547991504243258144.3688749835255860442.&amp;#34;</code></pre>
</div>
<p><strong>The DNS plugin was detecting a loop.</strong></p>
<p>This meant the network configuration was fundamentally broken.</p>
<h2 id="845-am-the-root-cause-claudes-analysis">8:45 AM: The Root Cause (Claude&rsquo;s Analysis)<a hidden class="anchor" aria-hidden="true" href="#845-am-the-root-cause-claudes-analysis">#</a></h2>
<p><em>Vault Evidence: The K3s fix file shows &ldquo;Root Cause: DHCP IP address changed&rdquo; with exact IPs: &ldquo;K3s cached IP: 192.168.1.79 (old)&rdquo; and &ldquo;Current wired IP: 192.168.1.186&rdquo;.</em></p>
<p>Working with Claude to check the CNI (Container Network Interface) configuration, we found the issue:</p>
<div class="highlight">
  <pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code># Check network interfaces (Claude&amp;#39;s command)
ip addr show

# Output showed:
enp6s0:  192.168.1.186/24 (wired, not default)
wlp5s0:  192.168.1.180/24 (WiFi, IS default route)</code></pre>
</div>
<p><strong>The Problem</strong>:</p>
<ul>
<li>K3s was configured for IP: <code>192.168.1.79</code></li>
<li>Current IP was: <code>192.168.1.186</code> (wired) or <code>192.168.1.180</code> (WiFi)</li>
<li><strong>DHCP had reassigned the IP address</strong></li>
</ul>
<p><strong>When this happened</strong>: &ldquo;late September&rdquo; (exact date unknown)
<strong>How long it ran broken</strong>: 6,812 restart attempts = days or weeks</p>
<p>Claude explained: <em>&ldquo;K3s&rsquo;s Flannel CNI relies on host network interfaces. When the host IP changes, Flannel&rsquo;s cached network configuration becomes invalid, causing the DNS loop.&rdquo;</em></p>
<h2 id="900-am-the-fix-collaborative-debugging">9:00 AM: The Fix (Collaborative Debugging)<a hidden class="anchor" aria-hidden="true" href="#900-am-the-fix-collaborative-debugging">#</a></h2>
<p><em>Vault Evidence: The fix file shows three solution options with Claude&rsquo;s analysis of pros/cons for each.</em></p>
<p>Claude and I discussed three solutions:</p>
<p><strong>Option 1: Clean Restart</strong> (Recommended by Claude)</p>
<div class="highlight">
  <pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code>sudo systemctl stop k3s
sudo rm -f /var/lib/rancher/k3s/server/cred/node-passwd
sudo systemctl start k3s</code></pre>
</div>
<p><strong>Option 2: Pin to Specific IP</strong> (Claude&rsquo;s alternative)</p>
<div class="highlight">
  <pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code>sudo vi /etc/systemd/system/k3s.service
# Add: --node-ip=192.168.1.180
sudo systemctl daemon-reload
sudo systemctl restart k3s</code></pre>
</div>
<p><strong>Option 3: Pin to Interface</strong> (Claude&rsquo;s best long-term solution)</p>
<div class="highlight">
  <pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code>sudo vi /etc/systemd/system/k3s.service
# Add: --flannel-iface=wlp5s0
sudo systemctl daemon-reload
sudo systemctl restart k3s</code></pre>
</div>
<p>We went with <strong>Option 1</strong> for immediate recovery, then implemented <strong>Option 3</strong> for long-term stability.</p>
<h2 id="915-am-the-recovery">9:15 AM: The Recovery<a hidden class="anchor" aria-hidden="true" href="#915-am-the-recovery">#</a></h2>
<div class="highlight">
  <pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code># Stop K3s
sudo systemctl stop k3s

# Remove cached network credentials
sudo rm -f /var/lib/rancher/k3s/server/cred/node-passwd

# Start K3s (will re-detect network)
sudo systemctl start k3s

# Wait 60 seconds
sleep 60

# Verify
kubectl get nodes
kubectl get pods -A</code></pre>
</div>
<p><strong>Result</strong>:</p>
<div class="highlight">
  <pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code>NAME          STATUS   ROLES                  AGE   VERSION
leveling-pc   Ready    control-plane,master   3d    v1.30.5&#43;k3s1</code></pre>
</div>
<p><strong>Node: Ready</strong> ✅</p>
<p><strong>Pods starting</strong>:</p>
<div class="highlight">
  <pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code>NAMESPACE     NAME                          READY   STATUS    RESTARTS   AGE
convocanvas   convocanvas-7d4b9c8f6-xk2p9   1/1     Running   0          15s
chromadb      chromadb-0                    1/1     Running   0          18s
monitoring    prometheus-server-0           1/1     Running   0          12s
...</code></pre>
</div>
<p><strong>ALL PODS HEALTHY</strong> ✅</p>
<p>The cluster was back.</p>
<h2 id="1000-am-implementing-the-permanent-fix">10:00 AM: Implementing the Permanent Fix<a hidden class="anchor" aria-hidden="true" href="#1000-am-implementing-the-permanent-fix">#</a></h2>
<p>Working with Claude, I implemented Option 3 to prevent this from happening again:</p>
<div class="highlight">
  <pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code># Edit K3s service
sudo vi /etc/systemd/system/k3s.service

# Added this flag to ExecStart line:
--flannel-iface=wlp5s0 \

# Reload and restart
sudo systemctl daemon-reload
sudo systemctl restart k3s

# Verify everything still works
kubectl get nodes
kubectl get pods -A</code></pre>
</div>
<p><strong>Result</strong>: K3s now pinned to the WiFi interface. Future IP changes won&rsquo;t break the cluster.</p>
<h2 id="what-worked">What Worked<a hidden class="anchor" aria-hidden="true" href="#what-worked">#</a></h2>
<p><strong>Systematic Debugging</strong> (Claude&rsquo;s approach):</p>
<ol>
<li>Check node status</li>
<li>Check pod logs</li>
<li>Check DNS resolution</li>
<li>Check CoreDNS status</li>
<li>Check network configuration</li>
<li>Identify root cause</li>
<li>Implement fix</li>
</ol>
<p><strong>Collaboration with Claude</strong>: The debugging pattern, solution options analysis, and long-term fix recommendations came from working together. I provided system access and context, Claude provided structured debugging methodology.</p>
<p><strong>Clean Recovery</strong>: Option 1 (clean restart) got the cluster running in minutes.</p>
<p><strong>Permanent Fix</strong>: Option 3 (interface pinning) prevents recurrence.</p>
<h2 id="what-still-sucked">What Still Sucked<a hidden class="anchor" aria-hidden="true" href="#what-still-sucked">#</a></h2>
<p><strong>Silent Failure</strong>: 6,812 restarts over days/weeks with no alerting. I had no idea the cluster was broken.</p>
<p><strong>No Monitoring</strong>: Should have had alerts on pod restart counts &gt;10.</p>
<p><strong>DHCP Dependency</strong>: Infrastructure relying on DHCP is fragile. Static IPs would prevent this.</p>
<p><strong>Lost Time</strong>: Unknown how long the cluster was actually down. Could have been days of lost service.</p>
<h2 id="the-numbers-october-5-2025">The Numbers (October 5, 2025)<a hidden class="anchor" aria-hidden="true" href="#the-numbers-october-5-2025">#</a></h2>
<table>
  <thead>
      <tr>
          <th>Metric</th>
          <th>Value</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td><strong>Discovery Time</strong></td>
          <td>08:23 AM (Saturday)</td>
      </tr>
      <tr>
          <td><strong>Total Restarts</strong></td>
          <td>6,812</td>
      </tr>
      <tr>
          <td><strong>Diagnosis Time</strong></td>
          <td>22 minutes (08:23-08:45)</td>
      </tr>
      <tr>
          <td><strong>Recovery Time</strong></td>
          <td>30 minutes (09:00-09:30)</td>
      </tr>
      <tr>
          <td><strong>Permanent Fix</strong></td>
          <td>30 minutes (10:00-10:30)</td>
      </tr>
      <tr>
          <td><strong>Total Downtime</strong></td>
          <td>Unknown (days/weeks)</td>
      </tr>
      <tr>
          <td><strong>Detection to Recovery</strong></td>
          <td>~1 hour</td>
      </tr>
      <tr>
          <td><strong>Services Affected</strong></td>
          <td>All K3s workloads (LibreChat, RAG, Prometheus, Grafana, ELK, Jaeger, Kafka)</td>
      </tr>
      <tr>
          <td><strong>Services Unaffected</strong></td>
          <td>Host services (Ollama, FastMCP, GPU Monitor)</td>
      </tr>
  </tbody>
</table>
<p><code>★ Insight ─────────────────────────────────────</code>
<strong>Infrastructure Resilience Lessons:</strong></p>
<p>This K3s crash taught critical lessons about production infrastructure:</p>
<ol>
<li>
<p><strong>Alerting is Non-Negotiable</strong>: 6,812 restarts should have triggered alerts at restart #10. Silent failures are the worst kind.</p>
</li>
<li>
<p><strong>Network Dependencies Kill Clusters</strong>: Depending on DHCP for infrastructure IPs introduces fragility. Static IPs or interface pinning prevents this.</p>
</li>
<li>
<p><strong>Systematic Debugging Saves Time</strong>: Working with Claude&rsquo;s structured approach (node→pods→logs→DNS→network) found the root cause in 22 minutes.</p>
</li>
<li>
<p><strong>Collaboration Accelerates Recovery</strong>: Human access + AI patterns = faster diagnosis than either alone.</p>
</li>
<li>
<p><strong>Permanent Fixes &gt; Quick Fixes</strong>: Option 1 got us running, but Option 3 prevented future failures. Both matter.</p>
</li>
</ol>
<p><strong>The real problem wasn&rsquo;t the crash - it was not knowing it had crashed until manual inspection.</strong>
<code>─────────────────────────────────────────────────</code></p>
<h2 id="what-i-learned">What I Learned<a hidden class="anchor" aria-hidden="true" href="#what-i-learned">#</a></h2>
<p><strong>1. Working with Claude for infrastructure debugging is powerful</strong>
Structured debugging patterns + system knowledge = fast root cause identification.</p>
<p><strong>2. Saturday morning discoveries are better than Monday</strong>
Finding this on a weekend meant time to fix properly instead of quick patches before work.</p>
<p><strong>3. DHCP is fine for laptops, not for infrastructure</strong>
K3s clusters need stable IPs. Either static assignment or interface pinning.</p>
<p><strong>4. Monitoring gaps are invisible until failure</strong>
Everything seemed fine&hellip; until I manually checked. Alerts would have caught this days earlier.</p>
<p><strong>5. The cluster rebuild was the easy part</strong>
The hard part was discovering the issue and diagnosing root cause. Recovery took 30 minutes; diagnosis took longer.</p>
<h2 id="whats-next">What&rsquo;s Next<a hidden class="anchor" aria-hidden="true" href="#whats-next">#</a></h2>
<p>October 5, 10:30 AM. K3s was back. All pods healthy. The cluster was resilient again.</p>
<p><strong>But the system still couldn&rsquo;t document itself.</strong></p>
<p>With 23 pods running, multiple services deployed, and complex networking, I needed <strong>automated architecture diagrams</strong> that updated themselves.</p>
<p>By October 7, working with Claude, I&rsquo;d build exactly that.</p>
<hr>
<p><em>This is Episode 6 of &ldquo;Season 1: From Zero to Automated Infrastructure&rdquo; - documenting the Saturday morning crash that tested everything.</em></p>
<p><em>Previous Episode</em>: <a href="season-1-episode-5-migration-question">The Migration Question</a>
<em>Next Episode</em>: <a href="season-1-episode-7-diagram-automation">Teaching the System to Document Itself</a>
<em>Complete Series</em>: <a href="/tags/season-1/">Season 1 Mapping Report</a></p>
<hr>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
      <li><a href="https://blog.rduffy.uk/tags/k3s/">K3s</a></li>
      <li><a href="https://blog.rduffy.uk/tags/kubernetes/">Kubernetes</a></li>
      <li><a href="https://blog.rduffy.uk/tags/debugging/">Debugging</a></li>
      <li><a href="https://blog.rduffy.uk/tags/disaster-recovery/">Disaster-Recovery</a></li>
      <li><a href="https://blog.rduffy.uk/tags/infrastructure/">Infrastructure</a></li>
      <li><a href="https://blog.rduffy.uk/tags/collaboration/">Collaboration</a></li>
    </ul>
<nav class="paginav">
  <a class="prev" href="https://blog.rduffy.uk/posts/season-1-episode-5-migration-question/">
    <span class="title">« Prev</span>
    <br>
    <span>The Migration Question: When K3s Meets Reality</span>
  </a>
</nav>


<ul class="share-buttons">
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share When Everything Crashes: The K3s Resurrection on x"
            href="https://x.com/intent/tweet/?text=When%20Everything%20Crashes%3a%20The%20K3s%20Resurrection&amp;url=https%3a%2f%2fblog.rduffy.uk%2fposts%2fseason-1-episode-6-k3s-crash-resurrection%2f&amp;hashtags=k3s%2ckubernetes%2cdebugging%2cdisaster-recovery%2cinfrastructure%2ccollaboration">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M512 62.554 L 512 449.446 C 512 483.97 483.97 512 449.446 512 L 62.554 512 C 28.03 512 0 483.97 0 449.446 L 0 62.554 C 0 28.03 28.029 0 62.554 0 L 449.446 0 C 483.971 0 512 28.03 512 62.554 Z M 269.951 190.75 L 182.567 75.216 L 56 75.216 L 207.216 272.95 L 63.9 436.783 L 125.266 436.783 L 235.9 310.383 L 332.567 436.783 L 456 436.783 L 298.367 228.367 L 432.367 75.216 L 371.033 75.216 Z M 127.633 110 L 164.101 110 L 383.481 400.065 L 349.5 400.065 Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share When Everything Crashes: The K3s Resurrection on linkedin"
            href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2fblog.rduffy.uk%2fposts%2fseason-1-episode-6-k3s-crash-resurrection%2f&amp;title=When%20Everything%20Crashes%3a%20The%20K3s%20Resurrection&amp;summary=When%20Everything%20Crashes%3a%20The%20K3s%20Resurrection&amp;source=https%3a%2f%2fblog.rduffy.uk%2fposts%2fseason-1-episode-6-k3s-crash-resurrection%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-288.985,423.278l0,-225.717l-75.04,0l0,225.717l75.04,0Zm270.539,0l0,-129.439c0,-69.333 -37.018,-101.586 -86.381,-101.586c-39.804,0 -57.634,21.891 -67.617,37.266l0,-31.958l-75.021,0c0.995,21.181 0,225.717 0,225.717l75.02,0l0,-126.056c0,-6.748 0.486,-13.492 2.474,-18.315c5.414,-13.475 17.767,-27.434 38.494,-27.434c27.135,0 38.007,20.707 38.007,51.037l0,120.768l75.024,0Zm-307.552,-334.556c-25.674,0 -42.448,16.879 -42.448,39.002c0,21.658 16.264,39.002 41.455,39.002l0.484,0c26.165,0 42.452,-17.344 42.452,-39.002c-0.485,-22.092 -16.241,-38.954 -41.943,-39.002Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share When Everything Crashes: The K3s Resurrection on reddit"
            href="https://reddit.com/submit?url=https%3a%2f%2fblog.rduffy.uk%2fposts%2fseason-1-episode-6-k3s-crash-resurrection%2f&title=When%20Everything%20Crashes%3a%20The%20K3s%20Resurrection">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-3.446,265.638c0,-22.964 -18.616,-41.58 -41.58,-41.58c-11.211,0 -21.361,4.457 -28.841,11.666c-28.424,-20.508 -67.586,-33.757 -111.204,-35.278l18.941,-89.121l61.884,13.157c0.756,15.734 13.642,28.29 29.56,28.29c16.407,0 29.706,-13.299 29.706,-29.701c0,-16.403 -13.299,-29.702 -29.706,-29.702c-11.666,0 -21.657,6.792 -26.515,16.578l-69.105,-14.69c-1.922,-0.418 -3.939,-0.042 -5.585,1.036c-1.658,1.073 -2.811,2.761 -3.224,4.686l-21.152,99.438c-44.258,1.228 -84.046,14.494 -112.837,35.232c-7.468,-7.164 -17.589,-11.591 -28.757,-11.591c-22.965,0 -41.585,18.616 -41.585,41.58c0,16.896 10.095,31.41 24.568,37.918c-0.639,4.135 -0.99,8.328 -0.99,12.576c0,63.977 74.469,115.836 166.33,115.836c91.861,0 166.334,-51.859 166.334,-115.836c0,-4.218 -0.347,-8.387 -0.977,-12.493c14.564,-6.47 24.735,-21.034 24.735,-38.001Zm-119.474,108.193c-20.27,20.241 -59.115,21.816 -70.534,21.816c-11.428,0 -50.277,-1.575 -70.522,-21.82c-3.007,-3.008 -3.007,-7.882 0,-10.889c3.003,-2.999 7.882,-3.003 10.885,0c12.777,12.781 40.11,17.317 59.637,17.317c19.522,0 46.86,-4.536 59.657,-17.321c3.016,-2.999 7.886,-2.995 10.885,0.008c3.008,3.011 3.003,7.882 -0.008,10.889Zm-5.23,-48.781c-16.373,0 -29.701,-13.324 -29.701,-29.698c0,-16.381 13.328,-29.714 29.701,-29.714c16.378,0 29.706,13.333 29.706,29.714c0,16.374 -13.328,29.698 -29.706,29.698Zm-160.386,-29.702c0,-16.381 13.328,-29.71 29.714,-29.71c16.369,0 29.689,13.329 29.689,29.71c0,16.373 -13.32,29.693 -29.689,29.693c-16.386,0 -29.714,-13.32 -29.714,-29.693Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share When Everything Crashes: The K3s Resurrection on facebook"
            href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fblog.rduffy.uk%2fposts%2fseason-1-episode-6-k3s-crash-resurrection%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-106.468,0l0,-192.915l66.6,0l12.672,-82.621l-79.272,0l0,-53.617c0,-22.603 11.073,-44.636 46.58,-44.636l36.042,0l0,-70.34c0,0 -32.71,-5.582 -63.982,-5.582c-65.288,0 -107.96,39.569 -107.96,111.204l0,62.971l-72.573,0l0,82.621l72.573,0l0,192.915l-191.104,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share When Everything Crashes: The K3s Resurrection on whatsapp"
            href="https://api.whatsapp.com/send?text=When%20Everything%20Crashes%3a%20The%20K3s%20Resurrection%20-%20https%3a%2f%2fblog.rduffy.uk%2fposts%2fseason-1-episode-6-k3s-crash-resurrection%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-58.673,127.703c-33.842,-33.881 -78.847,-52.548 -126.798,-52.568c-98.799,0 -179.21,80.405 -179.249,179.234c-0.013,31.593 8.241,62.428 23.927,89.612l-25.429,92.884l95.021,-24.925c26.181,14.28 55.659,21.807 85.658,21.816l0.074,0c98.789,0 179.206,-80.413 179.247,-179.243c0.018,-47.895 -18.61,-92.93 -52.451,-126.81Zm-126.797,275.782l-0.06,0c-26.734,-0.01 -52.954,-7.193 -75.828,-20.767l-5.441,-3.229l-56.386,14.792l15.05,-54.977l-3.542,-5.637c-14.913,-23.72 -22.791,-51.136 -22.779,-79.287c0.033,-82.142 66.867,-148.971 149.046,-148.971c39.793,0.014 77.199,15.531 105.329,43.692c28.128,28.16 43.609,65.592 43.594,105.4c-0.034,82.149 -66.866,148.983 -148.983,148.984Zm81.721,-111.581c-4.479,-2.242 -26.499,-13.075 -30.604,-14.571c-4.105,-1.495 -7.091,-2.241 -10.077,2.241c-2.986,4.483 -11.569,14.572 -14.182,17.562c-2.612,2.988 -5.225,3.364 -9.703,1.12c-4.479,-2.241 -18.91,-6.97 -36.017,-22.23c-13.314,-11.876 -22.304,-26.542 -24.916,-31.026c-2.612,-4.484 -0.279,-6.908 1.963,-9.14c2.016,-2.007 4.48,-5.232 6.719,-7.847c2.24,-2.615 2.986,-4.484 4.479,-7.472c1.493,-2.99 0.747,-5.604 -0.374,-7.846c-1.119,-2.241 -10.077,-24.288 -13.809,-33.256c-3.635,-8.733 -7.327,-7.55 -10.077,-7.688c-2.609,-0.13 -5.598,-0.158 -8.583,-0.158c-2.986,0 -7.839,1.121 -11.944,5.604c-4.105,4.484 -15.675,15.32 -15.675,37.364c0,22.046 16.048,43.342 18.287,46.332c2.24,2.99 31.582,48.227 76.511,67.627c10.685,4.615 19.028,7.371 25.533,9.434c10.728,3.41 20.492,2.929 28.209,1.775c8.605,-1.285 26.499,-10.833 30.231,-21.295c3.732,-10.464 3.732,-19.431 2.612,-21.298c-1.119,-1.869 -4.105,-2.99 -8.583,-5.232Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share When Everything Crashes: The K3s Resurrection on telegram"
            href="https://telegram.me/share/url?text=When%20Everything%20Crashes%3a%20The%20K3s%20Resurrection&amp;url=https%3a%2f%2fblog.rduffy.uk%2fposts%2fseason-1-episode-6-k3s-crash-resurrection%2f">
            <svg version="1.1" xml:space="preserve" viewBox="2 2 28 28" height="30px" width="30px" fill="currentColor">
                <path
                    d="M26.49,29.86H5.5a3.37,3.37,0,0,1-2.47-1,3.35,3.35,0,0,1-1-2.47V5.48A3.36,3.36,0,0,1,3,3,3.37,3.37,0,0,1,5.5,2h21A3.38,3.38,0,0,1,29,3a3.36,3.36,0,0,1,1,2.46V26.37a3.35,3.35,0,0,1-1,2.47A3.38,3.38,0,0,1,26.49,29.86Zm-5.38-6.71a.79.79,0,0,0,.85-.66L24.73,9.24a.55.55,0,0,0-.18-.46.62.62,0,0,0-.41-.17q-.08,0-16.53,6.11a.59.59,0,0,0-.41.59.57.57,0,0,0,.43.52l4,1.24,1.61,4.83a.62.62,0,0,0,.63.43.56.56,0,0,0,.4-.17L16.54,20l4.09,3A.9.9,0,0,0,21.11,23.15ZM13.8,20.71l-1.21-4q8.72-5.55,8.78-5.55c.15,0,.23,0,.23.16a.18.18,0,0,1,0,.06s-2.51,2.3-7.52,6.8Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share When Everything Crashes: The K3s Resurrection on ycombinator"
            href="https://news.ycombinator.com/submitlink?t=When%20Everything%20Crashes%3a%20The%20K3s%20Resurrection&u=https%3a%2f%2fblog.rduffy.uk%2fposts%2fseason-1-episode-6-k3s-crash-resurrection%2f">
            <svg version="1.1" xml:space="preserve" width="30px" height="30px" viewBox="0 0 512 512" fill="currentColor"
                xmlns:inkscape="http://www.inkscape.org/namespaces/inkscape">
                <path
                    d="M449.446 0C483.971 0 512 28.03 512 62.554L512 449.446C512 483.97 483.97 512 449.446 512L62.554 512C28.03 512 0 483.97 0 449.446L0 62.554C0 28.03 28.029 0 62.554 0L449.446 0ZM183.8767 87.9921H121.8427L230.6673 292.4508V424.0079H281.3328V292.4508L390.1575 87.9921H328.1233L256 238.2489z" />
            </svg>
        </a>
    </li>
</ul>

  </footer>
</article>
    </main>
    
<footer class="footer">
        <span>&copy; 2025 <a href="https://blog.rduffy.uk/">Ryan Duffy - AI Infrastructure &amp; Local LLM Journey</a></span> · 

    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
<script>
    document.querySelectorAll('pre > code').forEach((codeblock) => {
        const container = codeblock.parentNode.parentNode;

        const copybutton = document.createElement('button');
        copybutton.classList.add('copy-code');
        copybutton.innerHTML = 'copy';

        function copyingDone() {
            copybutton.innerHTML = 'copied!';
            setTimeout(() => {
                copybutton.innerHTML = 'copy';
            }, 2000);
        }

        copybutton.addEventListener('click', (cb) => {
            if ('clipboard' in navigator) {
                navigator.clipboard.writeText(codeblock.textContent);
                copyingDone();
                return;
            }

            const range = document.createRange();
            range.selectNodeContents(codeblock);
            const selection = window.getSelection();
            selection.removeAllRanges();
            selection.addRange(range);
            try {
                document.execCommand('copy');
                copyingDone();
            } catch (e) { };
            selection.removeRange(range);
        });

        if (container.classList.contains("highlight")) {
            container.appendChild(copybutton);
        } else if (container.parentNode.firstChild == container) {
            
        } else if (codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName == "TABLE") {
            
            codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(copybutton);
        } else {
            
            codeblock.parentNode.appendChild(copybutton);
        }
    });
</script>
</body>

</html>
