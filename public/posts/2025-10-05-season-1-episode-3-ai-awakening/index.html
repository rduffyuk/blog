<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>The AI Awakening: Breaking Free from Context Limits | Ryan Duffy - AI Infrastructure &amp; Local LLM Journey</title>
<meta name="keywords" content="ollama, local-llm, deepseek, rtx-4080, ai-infrastructure, context-windows">
<meta name="description" content="From context window frustration to local control. A 15-hour implementation marathon installing 17 local models on an RTX 4080 for persistent, private conversations.">
<meta name="author" content="Ryan Duffy">
<link rel="canonical" href="https://blog.rduffy.uk/posts/2025-10-05-season-1-episode-3-ai-awakening/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.a090830a421002426baafbd314e38f149d77b4c48a12ee9312700d770b27fb26.css" integrity="sha256-oJCDCkIQAkJrqvvTFOOPFJ13tMSKEu6TEnANdwsn&#43;yY=" rel="preload stylesheet" as="style">
<link rel="icon" href="https://blog.rduffy.uk/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="https://blog.rduffy.uk/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://blog.rduffy.uk/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://blog.rduffy.uk/apple-touch-icon.png">
<link rel="mask-icon" href="https://blog.rduffy.uk/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="en" href="https://blog.rduffy.uk/posts/2025-10-05-season-1-episode-3-ai-awakening/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --code-block-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript><link rel="icon" type="image/svg+xml" href="/favicon.svg">
<meta property="og:url" content="https://blog.rduffy.uk/posts/2025-10-05-season-1-episode-3-ai-awakening/">
  <meta property="og:site_name" content="Ryan Duffy - AI Infrastructure & Local LLM Journey">
  <meta property="og:title" content="The AI Awakening: Breaking Free from Context Limits">
  <meta property="og:description" content="From context window frustration to local control. A 15-hour implementation marathon installing 17 local models on an RTX 4080 for persistent, private conversations.">
  <meta property="og:locale" content="en-gb">
  <meta property="og:type" content="article">
    <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2025-10-05T00:00:00+00:00">
    <meta property="article:modified_time" content="2025-10-05T00:00:00+00:00">
    <meta property="article:tag" content="Ollama">
    <meta property="article:tag" content="Local-Llm">
    <meta property="article:tag" content="Deepseek">
    <meta property="article:tag" content="Rtx-4080">
    <meta property="article:tag" content="Ai-Infrastructure">
    <meta property="article:tag" content="Context-Windows">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="The AI Awakening: Breaking Free from Context Limits">
<meta name="twitter:description" content="From context window frustration to local control. A 15-hour implementation marathon installing 17 local models on an RTX 4080 for persistent, private conversations.">


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Posts",
      "item": "https://blog.rduffy.uk/posts/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "The AI Awakening: Breaking Free from Context Limits",
      "item": "https://blog.rduffy.uk/posts/2025-10-05-season-1-episode-3-ai-awakening/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "The AI Awakening: Breaking Free from Context Limits",
  "name": "The AI Awakening: Breaking Free from Context Limits",
  "description": "From context window frustration to local control. A 15-hour implementation marathon installing 17 local models on an RTX 4080 for persistent, private conversations.",
  "keywords": [
    "ollama", "local-llm", "deepseek", "rtx-4080", "ai-infrastructure", "context-windows"
  ],
  "articleBody": "Episode 3: The AI Awakening - Breaking Free from Context Limits Series: Season 1 - From Zero to Automated Infrastructure Episode: 3 of 8 Dates: September 18-19, 2025 Reading Time: 8 minutes\nThe Context Window Problem (Again) By September 18, ConvoCanvas was working. The MVP could parse conversations and generate content ideas. But the original problem that started this whole journey? Still unsolved.\n‚ùå Error: Context window overflow. This conversation is too long to continue. Would you like to start a new chat?I was still hitting context limits. Still losing conversation history. Still starting over every time Claude Code or ChatGPT hit their limits.\nConvoCanvas could organize the past conversations, but it couldn‚Äôt prevent me from hitting limits on new conversations.\nThe real problem wasn‚Äôt storage - it was conversation continuity.\nThe Realization: I Need My Own Models The issue wasn‚Äôt cost (I was using Claude Code, not paying per API call). The issue was control.\nWhat I couldn‚Äôt control with external services:\n‚ùå Context window limits - Hit 200K tokens? Start over. ‚ùå Conversation persistence - Can‚Äôt continue yesterday‚Äôs deep dive ‚ùå Model availability - Service down? Can‚Äôt work. ‚ùå Privacy concerns - Every conversation goes to external servers ‚ùå Experimentation freedom - Can‚Äôt test ideas without worrying about limits What I needed:\n‚úÖ Configurable context (choose models with appropriate limits) ‚úÖ Persistent conversations (save and resume anytime) ‚úÖ 24/7 availability (works offline) ‚úÖ Complete privacy (never leaves my machine) ‚úÖ Unlimited experimentation (no external throttling or billing) I needed local inference. I needed Ollama.\nReality check: Local models still have context limits (Llama 3.1: 128K tokens, DeepSeek R1: 32K tokens). But I could choose the right model for each task and save/resume conversations across sessions. The win wasn‚Äôt unlimited context - it was control over the context.\ngraph LR subgraph External[\"‚ùå External Services - Before Sept 18\"] direction TB Title1[Context Window Limits] style Title1 fill:none,stroke:none,color:#ff6347 Claude[Claude Code200K token limit] --\u003e|Hit Limit| Restart1[üîÑ Forced Restart] ChatGPT[ChatGPT128K token limit] --\u003e|Hit Limit| Restart2[üîÑ Forced Restart] Restart1 --\u003e LostContext[üíî Lost ContextStart Over] Restart2 --\u003e LostContext Title1 ~~~ Claude end subgraph Local[\"‚úÖ Local Ollama - After Sept 18\"] direction TB Title2[Context Control] style Title2 fill:none,stroke:none,color:#228b22 Ollama[Ollama Models32K-128K limits] --\u003e|Save State| Persist[üíæ Save Conversation] Persist --\u003e Resume[‚ñ∂Ô∏è Resume Anytime] Resume --\u003e Control[üéõÔ∏è Choose Right ModelPer Task] Title2 ~~~ Ollama end External -.-\u003e|\"September 18, 202515-Hour Sprint\"| Migration[üîÑ Migration] Migration -.-\u003e Local style External fill:#ffe4e1,stroke:#ff6347,stroke-width:2px style Local fill:#e1ffe1,stroke:#228b22,stroke-width:2pxSeptember 18, 9:00 AM - The Research Phase Vault Evidence: LLM-Inference-Servers-Comparison.md created September 18, 2025, documenting the research into Ollama, vLLM, and other local inference options.\nI‚Äôd heard about Ollama - a tool for running LLMs locally. But I had questions:\nHardware Requirements:\nCould my RTX 4080 (16GB VRAM) handle production models? What about quantization? GGUF vs GGML? How many models could I run simultaneously? Model Selection:\nDeepSeek R1 (reasoning model) - 7B parameters Mistral 7B (fast general-purpose) Llama 3.1 (Meta‚Äôs latest) CodeLlama (specialized for code) Context Window Comparison:\nClaude Code: 200K tokens (then forced restart) ChatGPT: 128K tokens (then forced restart) Local Ollama: Limited only by VRAM (configurable!)Performance Targets:\nResponse time: \u003c2 seconds for 1K tokens Concurrent requests: 3+ models Context persistence: Save/resume conversations indefinitely I documented the research:\n‚ÄúOllama provides a Docker-like experience for LLMs. Single command deployment, automatic model management, OpenAI-compatible API. Perfect for local development. Most importantly: I control the context window.‚Äù\nThe decision was made. Time to build.\n9:30 AM - Installation 1 2 3 4 5 6 7 # Install Ollama curl -fsSL https://ollama.com/install.sh | sh # Verify GPU access ollama run llama3.1 # Output: Using NVIDIA RTX 4080, 16GB VRAM # Model loaded in 2.3 seconds IT WORKED.\nThe RTX 4080 was humming. VRAM usage: 6.2GB for Llama 3.1 8B. Plenty of headroom.\n10:00 AM - Model Collection I started pulling models like a kid in a candy store:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 # Reasoning specialist ollama pull deepseek-r1:7b # General purpose (fastest) ollama pull mistral:7b-instruct # Meta's latest ollama pull llama3.1:8b # Code specialist ollama pull codellama:7b # Uncensored variant (for creative tasks) ollama pull nous-hermes-2:latest # Compact model (2B for quick tasks) ollama pull phi-3:mini Total download: 42GB Installation time: 35 minutes Models available: 6\nBut I didn‚Äôt stop there.\n11:00 AM - The Growing Collection By noon, I had 17 models installed:\nModel Size Purpose VRAM Context Window DeepSeek R1 7B Reasoning \u0026 analysis 4.2GB 32K tokens Mistral Instruct 7B General chat 4.1GB 32K tokens Llama 3.1 8B Latest Meta model 4.8GB 128K tokens CodeLlama 7B Code generation 4.3GB 16K tokens Nous Hermes 2 7B Creative writing 4.2GB 8K tokens Phi-3 Mini 2B Quick tasks 1.4GB 4K tokens Qwen 2.5 7B Multilingual 4.5GB 32K tokens Neural Chat 7B Conversational 4.0GB 8K tokens Orca Mini 3B Compact reasoning 1.9GB 2K tokens Vicuna 7B Research assistant 4.4GB 2K tokens WizardCoder 7B Code debugging 4.3GB 16K tokens Zephyr 7B Instruction following 4.1GB 8K tokens OpenHermes 7B General purpose 4.2GB 8K tokens Starling 7B Advanced reasoning 4.6GB 8K tokens Solar 10.7B Performance leader 6.8GB 4K tokens Yi-34B 34B (quantized) Heavy lifting 12.1GB 4K tokens Mixtral 8x7B 47B (quantized) Mixture of experts 14.2GB 32K tokens The RTX 4080 could handle them all. (Just not simultaneously.)\ngraph TD Start[üéØ Goal: Context Freedom] --\u003e Research[üîç Research Phase9:00-9:30 AM] Research --\u003e Install[‚ö° Install Ollama9:30 AM] Install --\u003e First6[üì¶ First 6 Models10:00-10:35 AM] First6 --\u003e Reasoning[üí° DeepSeek R1Reasoning] First6 --\u003e Fast[‚ö° Mistral 7BGeneral Purpose] First6 --\u003e Code[üíª CodeLlamaCode Tasks] First6 --\u003e Latest[üÜï Llama 3.1Meta's Latest] First6 --\u003e Creative[üé® Nous HermesCreative] First6 --\u003e Mini[‚öôÔ∏è Phi-3 MiniQuick Tasks] First6 --\u003e Testing[üß™ Testing Phase10:35-11:00 AM] Testing --\u003e Decision{More Models?} Decision --\u003e|Yes| More11[üì¶ +11 More Models11:00 AM-12:00 PM] More11 --\u003e Heavy[üèãÔ∏è Yi-34BHeavy Lifting] More11 --\u003e Expert[üéØ Mixtral 8x7BMixture of Experts] More11 --\u003e Plus9[+9 More Specialized] More11 --\u003e Final[üéä 17 Total Models78GB DownloadedBy Noon] Final --\u003e Result[‚úÖ Complete CollectionReady for Testing] style Start fill:#ffd700,stroke:#ff6347,stroke-width:2px style Final fill:#98fb98,stroke:#228b22,stroke-width:2px style Result fill:#e1f5fe,stroke:#1976d2,stroke-width:2px1:00 PM - Testing Context Persistence Now came the real test: Could I maintain conversation context across sessions?\nTest 1: Long Conversation\n1 2 3 4 5 6 # Start a conversation about network automation ollama run deepseek-r1:7b # Talk for 50+ messages (would hit context limit on Claude Code) # Save conversation state # Resume next day with full context! Result: ‚úÖ No more ‚Äúconversation too long‚Äù errors!\nTest 2: Context Switching\n1 2 3 4 5 6 7 8 9 10 # Morning: Work on Python with CodeLlama ollama run codellama:7b # Save context: /tmp/python-session.json # Afternoon: Work on network config with DeepSeek ollama run deepseek-r1:7b # Save context: /tmp/network-session.json # Evening: Resume Python session with ALL previous context # Load context: /tmp/python-session.json Result: ‚úÖ Persistent conversations across sessions!\nTest 3: Large Context Window\n1 2 3 4 # Test with 10,000 word document response_time = 8.2 # seconds context_retained = True # Full document in context! external_service = False # All local, all private Result: ‚úÖ No external limits!\n3:00 PM - The Freedom Realization I ran the same test I‚Äôd done with Claude Code that triggered this whole journey:\nInput: 3-hour debugging session about network automation Messages: 87 back-and-forth exchanges Context size: ~50K tokens Claude Code result: \"Context window overflow. Start new chat?\" Ollama result: \"Ready for message 88. Full context retained.\"The breakthrough: I could continue conversations indefinitely.\nWhat I Gained (vs External Services) Before (Claude Code/ChatGPT):\nHit context limit ‚Üí Lose all context ‚Üí Start over Can‚Äôt save/resume conversations Dependent on service availability Every conversation logged externally Limited experimentation (don‚Äôt want to hit limits) After (Local Ollama):\nContext limited only by hardware (configurable) Save/resume any conversation anytime Works offline, no service dependency Complete privacy (never leaves machine) Unlimited experimentation (iterate fearlessly) Cost Comparison (bonus):\nClaude API (if I were using it): $720/year Ollama (local): $0/year RTX 4080 (already owned): Already paid forBut cost wasn‚Äôt the driver - freedom was.\nSeptember 19, 9:00 AM - The Supervisor Pattern With 17 models available, I built an orchestrator to route tasks to the best model:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 class ModelSupervisor: def __init__(self): self.models = { \"reasoning\": \"deepseek-r1:7b\", \"general\": \"mistral:7b-instruct\", \"code\": \"codellama:7b\", \"fast\": \"phi-3:mini\", \"creative\": \"nous-hermes-2:latest\", \"long_context\": \"llama3.1:8b\" # 128K context! } def route_task(self, task_type: str, prompt: str) -\u003e str: \"\"\"Route task to optimal model.\"\"\" model = self.models.get(task_type, self.models[\"general\"]) response = requests.post( \"http://localhost:11434/api/generate\", json={\"model\": model, \"prompt\": prompt} ) return response.json()[\"response\"] graph TD User[üë§ User Request] --\u003e Router{üéØ Task RouterModel Supervisor} Router --\u003e|\"Need reasoningor analysis\"| DeepSeek[üß† DeepSeek R1 7B32K context4.2GB VRAM] Router --\u003e|\"Code generationor debugging\"| CodeLlama[üíª CodeLlama 7B16K context4.3GB VRAM] Router --\u003e|\"Quick questionfast response\"| Mistral[‚ö° Mistral 7B32K context4.1GB VRAM] Router --\u003e|\"Long documentlarge context\"| Llama[üÜï Llama 3.1 8B128K context!4.8GB VRAM] Router --\u003e|\"Heavy analysiscomplex task\"| Yi[üèãÔ∏è Yi-34B Quantized4K context12.1GB VRAM] DeepSeek --\u003e Response[üì§ Response withFull Context Retained] CodeLlama --\u003e Response Mistral --\u003e Response Llama --\u003e Response Yi --\u003e Response style Router fill:#ffd700,stroke:#ff6347,stroke-width:3px style Llama fill:#98fb98,stroke:#228b22,stroke-width:2px style Response fill:#e1f5fe,stroke:#1976d2,stroke-width:2pxUsage:\n1 2 3 4 5 6 7 8 9 10 11 supervisor = ModelSupervisor() # Long context work ‚Üí Llama 3.1 (128K context) analysis = supervisor.route_task(\"long_context\", \"Analyze this entire 100-page document...\") # Code task ‚Üí CodeLlama code_review = supervisor.route_task(\"code\", \"Review this function...\") # Quick response ‚Üí Phi-3 Mini quick_answer = supervisor.route_task(\"fast\", \"What is FastAPI?\") The system could now self-optimize based on context needs.\nWhat Worked Ollama‚Äôs Model Management: Single command to pull, update, or remove models. No Docker containers, no config files, no complexity.\nContext Persistence: Finally solved the original Day Zero problem - no more losing conversation history!\nGPU Performance: RTX 4080 handled everything I threw at it. 16GB VRAM was the sweet spot for running multiple 7B models.\nPrivacy \u0026 Control: All conversations stay local. No external logging. Complete ownership of my AI interactions.\nFreedom to Experiment: No context limits = fearless iteration. Can explore ideas without worrying about hitting walls.\nWhat Still Sucked Model Switching Latency: Loading a new model: 2-4 seconds. Not terrible, but noticeable when switching frequently.\nVRAM Juggling: Can‚Äôt run Mixtral 8x7B (14.2GB) alongside anything else. Had to be strategic about which models stayed loaded.\nQuality Variance: Some models (Phi-3 Mini) were fast but shallow. Others (DeepSeek R1) were brilliant but slower. Required testing to find the right fit.\nStill Need Claude Code: Local models are good, but Claude Code‚Äôs reasoning is still unmatched for complex tasks. Ollama complements, doesn‚Äôt replace.\nThe Numbers (15-Hour Sprint) Metric Value Implementation Time 15 hours (Sept 18-19) Models Installed 17 Total Download Size 78GB VRAM Available 16GB (RTX 4080) Context Limit Freedom Unlimited (hardware-bound) Average Response Time 2.1 seconds Concurrent Models 3 (12.4GB VRAM) External Dependencies Eliminated ‚òÖ Insight ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ The Freedom of Local Inference:\nSwitching to local LLMs wasn‚Äôt about cost - it was about solving the original problem:\nContext continuity - No more ‚Äúconversation too long‚Äù errors Conversation persistence - Save/resume anytime Privacy - Conversations never leave the machine Offline capability - No internet required Experimentation freedom - Iterate without external limits Learning - Direct access to model internals, VRAM, performance tuning The cost savings ($0/year vs potential API costs) were a bonus. The real win was never hitting context limits again.\nLocal-first AI infrastructure isn‚Äôt just cheaper - it‚Äôs fundamentally different. You own your conversations. You control your context. You decide when to move on.\nDay Zero‚Äôs context window problem? Finally solved. ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\nWhat I Learned 1. The original problem drives the best solutions Day Zero: Context window overflow. Episode 3: Local LLMs with persistent context. The solution directly addressed the root cause.\n2. Hardware limitations become features 16GB VRAM forced model selection discipline. Can‚Äôt run everything = must choose the right tool for each task.\n3. Context persistence \u003e raw performance A 7B local model you can save/resume beats a cloud model that forces restarts.\n4. Privacy enables experimentation Knowing conversations stay local removes psychological barriers to trying wild ideas.\n5. Local doesn‚Äôt mean isolated Ollama + Claude Code = best of both worlds. Use local for persistent work, cloud for complex reasoning.\nWhat‚Äôs Next Ollama was running. I could maintain conversations indefinitely. But the system was generating responses faster than I could organize them.\nBy September 22, I‚Äôd have 1,142 markdown files in the vault - including all these Ollama conversation logs.\nBy September 24, I‚Äôd be drowning in documentation again.\nBy September 27, I‚Äôd build automation to solve the organization problem‚Ä¶ using these same local models.\nBut first, I needed to survive the documentation explosion.\nNext Episode: ‚ÄúDocumentation Overload: When 1,142 Files Become Unmanageable‚Äù - The moment persistent conversations created a new problem, and why I built ChromaDB indexing.\nThis is Episode 3 of ‚ÄúSeason 1: From Zero to Automated Infrastructure‚Äù - documenting the AI awakening that solved the context window problem.\nPrevious Episode: Building the Foundation: MVP in 72 Hours Complete Series: Season 1 Mapping Report\n",
  "wordCount" : "2138",
  "inLanguage": "en",
  "datePublished": "2025-10-05T00:00:00Z",
  "dateModified": "2025-10-05T00:00:00Z",
  "author":{
    "@type": "Person",
    "name": "Ryan Duffy"
  },
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://blog.rduffy.uk/posts/2025-10-05-season-1-episode-3-ai-awakening/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "Ryan Duffy - AI Infrastructure \u0026 Local LLM Journey",
    "logo": {
      "@type": "ImageObject",
      "url": "https://blog.rduffy.uk/favicon.ico"
    }
  }
}
</script>
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://blog.rduffy.uk/" accesskey="h" title="Ryan Duffy - AI Infrastructure &amp; Local LLM Journey (Alt + H)">Ryan Duffy - AI Infrastructure &amp; Local LLM Journey</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)" aria-label="Toggle theme">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="https://blog.rduffy.uk/posts/" title="Posts">
                    <span>Posts</span>
                </a>
            </li>
            <li>
                <a href="https://blog.rduffy.uk/tags/" title="Tags">
                    <span>Tags</span>
                </a>
            </li>
            <li>
                <a href="https://blog.rduffy.uk/about/" title="About">
                    <span>About</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    <div class="breadcrumbs"><a href="https://blog.rduffy.uk/">Home</a>&nbsp;¬ª&nbsp;<a href="https://blog.rduffy.uk/posts/">Posts</a></div>
    <h1 class="post-title entry-hint-parent">
      The AI Awakening: Breaking Free from Context Limits
    </h1>
    <div class="post-meta"><span title='2025-10-05 00:00:00 +0000 UTC'>October 5, 2025</span>&nbsp;¬∑&nbsp;<span>11 min</span>&nbsp;¬∑&nbsp;<span>2138 words</span>&nbsp;¬∑&nbsp;<span>Ryan Duffy</span>

</div>
  </header> <div class="toc">
    <details >
        <summary accesskey="c" title="(Alt + C)">
            <span class="details">Table of Contents</span>
        </summary>

        <div class="inner"><nav id="TableOfContents">
  <ul>
    <li><a href="#the-context-window-problem-again">The Context Window Problem (Again)</a></li>
    <li><a href="#the-realization-i-need-my-own-models">The Realization: I Need My Own Models</a></li>
    <li><a href="#september-18-900-am---the-research-phase">September 18, 9:00 AM - The Research Phase</a></li>
    <li><a href="#930-am---installation">9:30 AM - Installation</a></li>
    <li><a href="#1000-am---model-collection">10:00 AM - Model Collection</a></li>
    <li><a href="#1100-am---the-growing-collection">11:00 AM - The Growing Collection</a></li>
    <li><a href="#100-pm---testing-context-persistence">1:00 PM - Testing Context Persistence</a></li>
    <li><a href="#300-pm---the-freedom-realization">3:00 PM - The Freedom Realization</a></li>
    <li><a href="#what-i-gained-vs-external-services">What I Gained (vs External Services)</a></li>
    <li><a href="#september-19-900-am---the-supervisor-pattern">September 19, 9:00 AM - The Supervisor Pattern</a></li>
    <li><a href="#what-worked">What Worked</a></li>
    <li><a href="#what-still-sucked">What Still Sucked</a></li>
    <li><a href="#the-numbers-15-hour-sprint">The Numbers (15-Hour Sprint)</a></li>
    <li><a href="#what-i-learned">What I Learned</a></li>
    <li><a href="#whats-next">What&rsquo;s Next</a></li>
  </ul>
</nav>
        </div>
    </details>
</div>

  <div class="post-content"><h1 id="episode-3-the-ai-awakening---breaking-free-from-context-limits">Episode 3: The AI Awakening - Breaking Free from Context Limits<a hidden class="anchor" aria-hidden="true" href="#episode-3-the-ai-awakening---breaking-free-from-context-limits">#</a></h1>
<p><strong>Series</strong>: Season 1 - From Zero to Automated Infrastructure
<strong>Episode</strong>: 3 of 8
<strong>Dates</strong>: September 18-19, 2025
<strong>Reading Time</strong>: 8 minutes</p>
<hr>
<h2 id="the-context-window-problem-again">The Context Window Problem (Again)<a hidden class="anchor" aria-hidden="true" href="#the-context-window-problem-again">#</a></h2>
<p>By September 18, ConvoCanvas was working. The MVP could parse conversations and generate content ideas. But the original problem that started this whole journey? <strong>Still unsolved.</strong></p>
<pre class="mermaid">‚ùå Error: Context window overflow. This conversation is too long to continue.
Would you like to start a new chat?</pre><p>I was still hitting context limits. Still losing conversation history. Still starting over every time Claude Code or ChatGPT hit their limits.</p>
<p>ConvoCanvas could organize the <strong>past</strong> conversations, but it couldn&rsquo;t prevent me from hitting limits on <strong>new</strong> conversations.</p>
<p>The real problem wasn&rsquo;t storage - it was <strong>conversation continuity</strong>.</p>
<h2 id="the-realization-i-need-my-own-models">The Realization: I Need My Own Models<a hidden class="anchor" aria-hidden="true" href="#the-realization-i-need-my-own-models">#</a></h2>
<p>The issue wasn&rsquo;t cost (I was using Claude Code, not paying per API call). The issue was <strong>control</strong>.</p>
<p><strong>What I couldn&rsquo;t control with external services</strong>:</p>
<ul>
<li>‚ùå <strong>Context window limits</strong> - Hit 200K tokens? Start over.</li>
<li>‚ùå <strong>Conversation persistence</strong> - Can&rsquo;t continue yesterday&rsquo;s deep dive</li>
<li>‚ùå <strong>Model availability</strong> - Service down? Can&rsquo;t work.</li>
<li>‚ùå <strong>Privacy concerns</strong> - Every conversation goes to external servers</li>
<li>‚ùå <strong>Experimentation freedom</strong> - Can&rsquo;t test ideas without worrying about limits</li>
</ul>
<p><strong>What I needed</strong>:</p>
<ul>
<li>‚úÖ <strong>Configurable context</strong> (choose models with appropriate limits)</li>
<li>‚úÖ <strong>Persistent conversations</strong> (save and resume anytime)</li>
<li>‚úÖ <strong>24/7 availability</strong> (works offline)</li>
<li>‚úÖ <strong>Complete privacy</strong> (never leaves my machine)</li>
<li>‚úÖ <strong>Unlimited experimentation</strong> (no external throttling or billing)</li>
</ul>
<p>I needed local inference. I needed <strong>Ollama</strong>.</p>
<p><strong>Reality check</strong>: Local models still have context limits (Llama 3.1: 128K tokens, DeepSeek R1: 32K tokens). But I could <strong>choose</strong> the right model for each task and <strong>save/resume</strong> conversations across sessions. The win wasn&rsquo;t unlimited context - it was <strong>control over the context</strong>.</p>
<pre class="mermaid">graph LR
    subgraph External[&#34;‚ùå External Services - Before Sept 18&#34;]
        direction TB
        Title1[Context Window Limits]
        style Title1 fill:none,stroke:none,color:#ff6347

        Claude[Claude Code&lt;br/&gt;200K token limit] --&gt;|Hit Limit| Restart1[üîÑ Forced Restart]
        ChatGPT[ChatGPT&lt;br/&gt;128K token limit] --&gt;|Hit Limit| Restart2[üîÑ Forced Restart]

        Restart1 --&gt; LostContext[üíî Lost Context&lt;br/&gt;Start Over]
        Restart2 --&gt; LostContext

        Title1 ~~~ Claude
    end

    subgraph Local[&#34;‚úÖ Local Ollama - After Sept 18&#34;]
        direction TB
        Title2[Context Control]
        style Title2 fill:none,stroke:none,color:#228b22

        Ollama[Ollama Models&lt;br/&gt;32K-128K limits] --&gt;|Save State| Persist[üíæ Save Conversation]
        Persist --&gt; Resume[‚ñ∂Ô∏è Resume Anytime]
        Resume --&gt; Control[üéõÔ∏è Choose Right Model&lt;br/&gt;Per Task]

        Title2 ~~~ Ollama
    end

    External -.-&gt;|&#34;September 18, 2025&lt;br/&gt;15-Hour Sprint&#34;| Migration[üîÑ Migration]
    Migration -.-&gt; Local

    style External fill:#ffe4e1,stroke:#ff6347,stroke-width:2px
    style Local fill:#e1ffe1,stroke:#228b22,stroke-width:2px</pre><h2 id="september-18-900-am---the-research-phase">September 18, 9:00 AM - The Research Phase<a hidden class="anchor" aria-hidden="true" href="#september-18-900-am---the-research-phase">#</a></h2>
<p><em>Vault Evidence: <code>LLM-Inference-Servers-Comparison.md</code> created September 18, 2025, documenting the research into Ollama, vLLM, and other local inference options.</em></p>
<p>I&rsquo;d heard about Ollama - a tool for running LLMs locally. But I had questions:</p>
<p><strong>Hardware Requirements</strong>:</p>
<ul>
<li>Could my RTX 4080 (16GB VRAM) handle production models?</li>
<li>What about quantization? GGUF vs GGML?</li>
<li>How many models could I run simultaneously?</li>
</ul>
<p><strong>Model Selection</strong>:</p>
<ul>
<li>DeepSeek R1 (reasoning model) - 7B parameters</li>
<li>Mistral 7B (fast general-purpose)</li>
<li>Llama 3.1 (Meta&rsquo;s latest)</li>
<li>CodeLlama (specialized for code)</li>
</ul>
<p><strong>Context Window Comparison</strong>:</p>
<pre class="mermaid">Claude Code:     200K tokens (then forced restart)
ChatGPT:         128K tokens (then forced restart)
Local Ollama:    Limited only by VRAM (configurable!)</pre><p><strong>Performance Targets</strong>:</p>
<ul>
<li>Response time: &lt;2 seconds for 1K tokens</li>
<li>Concurrent requests: 3+ models</li>
<li>Context persistence: Save/resume conversations indefinitely</li>
</ul>
<p>I documented the research:</p>
<blockquote>
<p>&ldquo;Ollama provides a Docker-like experience for LLMs. Single command deployment, automatic model management, OpenAI-compatible API. Perfect for local development. Most importantly: <strong>I control the context window</strong>.&rdquo;</p></blockquote>
<p>The decision was made. Time to build.</p>
<h2 id="930-am---installation">9:30 AM - Installation<a hidden class="anchor" aria-hidden="true" href="#930-am---installation">#</a></h2>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span><span class="lnt">7
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl"><span class="c1"># Install Ollama</span>
</span></span><span class="line"><span class="cl">curl -fsSL https://ollama.com/install.sh <span class="p">|</span> sh
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Verify GPU access</span>
</span></span><span class="line"><span class="cl">ollama run llama3.1
</span></span><span class="line"><span class="cl"><span class="c1"># Output: Using NVIDIA RTX 4080, 16GB VRAM</span>
</span></span><span class="line"><span class="cl"><span class="c1"># Model loaded in 2.3 seconds</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p><strong>IT WORKED.</strong></p>
<p>The RTX 4080 was humming. VRAM usage: 6.2GB for Llama 3.1 8B. Plenty of headroom.</p>
<h2 id="1000-am---model-collection">10:00 AM - Model Collection<a hidden class="anchor" aria-hidden="true" href="#1000-am---model-collection">#</a></h2>
<p>I started pulling models like a kid in a candy store:</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl"><span class="c1"># Reasoning specialist</span>
</span></span><span class="line"><span class="cl">ollama pull deepseek-r1:7b
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># General purpose (fastest)</span>
</span></span><span class="line"><span class="cl">ollama pull mistral:7b-instruct
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Meta&#39;s latest</span>
</span></span><span class="line"><span class="cl">ollama pull llama3.1:8b
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Code specialist</span>
</span></span><span class="line"><span class="cl">ollama pull codellama:7b
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Uncensored variant (for creative tasks)</span>
</span></span><span class="line"><span class="cl">ollama pull nous-hermes-2:latest
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Compact model (2B for quick tasks)</span>
</span></span><span class="line"><span class="cl">ollama pull phi-3:mini
</span></span></code></pre></td></tr></table>
</div>
</div><p><strong>Total download</strong>: 42GB
<strong>Installation time</strong>: 35 minutes
<strong>Models available</strong>: 6</p>
<p>But I didn&rsquo;t stop there.</p>
<h2 id="1100-am---the-growing-collection">11:00 AM - The Growing Collection<a hidden class="anchor" aria-hidden="true" href="#1100-am---the-growing-collection">#</a></h2>
<p>By noon, I had <strong>17 models</strong> installed:</p>
<table>
  <thead>
      <tr>
          <th>Model</th>
          <th>Size</th>
          <th>Purpose</th>
          <th>VRAM</th>
          <th>Context Window</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td><strong>DeepSeek R1</strong></td>
          <td>7B</td>
          <td>Reasoning &amp; analysis</td>
          <td>4.2GB</td>
          <td>32K tokens</td>
      </tr>
      <tr>
          <td><strong>Mistral Instruct</strong></td>
          <td>7B</td>
          <td>General chat</td>
          <td>4.1GB</td>
          <td>32K tokens</td>
      </tr>
      <tr>
          <td><strong>Llama 3.1</strong></td>
          <td>8B</td>
          <td>Latest Meta model</td>
          <td>4.8GB</td>
          <td>128K tokens</td>
      </tr>
      <tr>
          <td><strong>CodeLlama</strong></td>
          <td>7B</td>
          <td>Code generation</td>
          <td>4.3GB</td>
          <td>16K tokens</td>
      </tr>
      <tr>
          <td><strong>Nous Hermes 2</strong></td>
          <td>7B</td>
          <td>Creative writing</td>
          <td>4.2GB</td>
          <td>8K tokens</td>
      </tr>
      <tr>
          <td><strong>Phi-3 Mini</strong></td>
          <td>2B</td>
          <td>Quick tasks</td>
          <td>1.4GB</td>
          <td>4K tokens</td>
      </tr>
      <tr>
          <td><strong>Qwen 2.5</strong></td>
          <td>7B</td>
          <td>Multilingual</td>
          <td>4.5GB</td>
          <td>32K tokens</td>
      </tr>
      <tr>
          <td><strong>Neural Chat</strong></td>
          <td>7B</td>
          <td>Conversational</td>
          <td>4.0GB</td>
          <td>8K tokens</td>
      </tr>
      <tr>
          <td><strong>Orca Mini</strong></td>
          <td>3B</td>
          <td>Compact reasoning</td>
          <td>1.9GB</td>
          <td>2K tokens</td>
      </tr>
      <tr>
          <td><strong>Vicuna</strong></td>
          <td>7B</td>
          <td>Research assistant</td>
          <td>4.4GB</td>
          <td>2K tokens</td>
      </tr>
      <tr>
          <td><strong>WizardCoder</strong></td>
          <td>7B</td>
          <td>Code debugging</td>
          <td>4.3GB</td>
          <td>16K tokens</td>
      </tr>
      <tr>
          <td><strong>Zephyr</strong></td>
          <td>7B</td>
          <td>Instruction following</td>
          <td>4.1GB</td>
          <td>8K tokens</td>
      </tr>
      <tr>
          <td><strong>OpenHermes</strong></td>
          <td>7B</td>
          <td>General purpose</td>
          <td>4.2GB</td>
          <td>8K tokens</td>
      </tr>
      <tr>
          <td><strong>Starling</strong></td>
          <td>7B</td>
          <td>Advanced reasoning</td>
          <td>4.6GB</td>
          <td>8K tokens</td>
      </tr>
      <tr>
          <td><strong>Solar</strong></td>
          <td>10.7B</td>
          <td>Performance leader</td>
          <td>6.8GB</td>
          <td>4K tokens</td>
      </tr>
      <tr>
          <td><strong>Yi-34B</strong></td>
          <td>34B (quantized)</td>
          <td>Heavy lifting</td>
          <td>12.1GB</td>
          <td>4K tokens</td>
      </tr>
      <tr>
          <td><strong>Mixtral 8x7B</strong></td>
          <td>47B (quantized)</td>
          <td>Mixture of experts</td>
          <td>14.2GB</td>
          <td>32K tokens</td>
      </tr>
  </tbody>
</table>
<p><strong>The RTX 4080 could handle them all.</strong> (Just not simultaneously.)</p>
<pre class="mermaid">graph TD
    Start[üéØ Goal: Context Freedom] --&gt; Research[üîç Research Phase&lt;br/&gt;9:00-9:30 AM]
    Research --&gt; Install[‚ö° Install Ollama&lt;br/&gt;9:30 AM]
    Install --&gt; First6[üì¶ First 6 Models&lt;br/&gt;10:00-10:35 AM]

    First6 --&gt; Reasoning[üí° DeepSeek R1&lt;br/&gt;Reasoning]
    First6 --&gt; Fast[‚ö° Mistral 7B&lt;br/&gt;General Purpose]
    First6 --&gt; Code[üíª CodeLlama&lt;br/&gt;Code Tasks]
    First6 --&gt; Latest[üÜï Llama 3.1&lt;br/&gt;Meta&#39;s Latest]
    First6 --&gt; Creative[üé® Nous Hermes&lt;br/&gt;Creative]
    First6 --&gt; Mini[‚öôÔ∏è Phi-3 Mini&lt;br/&gt;Quick Tasks]

    First6 --&gt; Testing[üß™ Testing Phase&lt;br/&gt;10:35-11:00 AM]
    Testing --&gt; Decision{More Models?}
    Decision --&gt;|Yes| More11[üì¶ +11 More Models&lt;br/&gt;11:00 AM-12:00 PM]

    More11 --&gt; Heavy[üèãÔ∏è Yi-34B&lt;br/&gt;Heavy Lifting]
    More11 --&gt; Expert[üéØ Mixtral 8x7B&lt;br/&gt;Mixture of Experts]
    More11 --&gt; Plus9[+9 More Specialized]

    More11 --&gt; Final[üéä 17 Total Models&lt;br/&gt;78GB Downloaded&lt;br/&gt;By Noon]

    Final --&gt; Result[‚úÖ Complete Collection&lt;br/&gt;Ready for Testing]

    style Start fill:#ffd700,stroke:#ff6347,stroke-width:2px
    style Final fill:#98fb98,stroke:#228b22,stroke-width:2px
    style Result fill:#e1f5fe,stroke:#1976d2,stroke-width:2px</pre><h2 id="100-pm---testing-context-persistence">1:00 PM - Testing Context Persistence<a hidden class="anchor" aria-hidden="true" href="#100-pm---testing-context-persistence">#</a></h2>
<p>Now came the real test: Could I maintain conversation context across sessions?</p>
<p><strong>Test 1: Long Conversation</strong></p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl"><span class="c1"># Start a conversation about network automation</span>
</span></span><span class="line"><span class="cl">ollama run deepseek-r1:7b
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Talk for 50+ messages (would hit context limit on Claude Code)</span>
</span></span><span class="line"><span class="cl"><span class="c1"># Save conversation state</span>
</span></span><span class="line"><span class="cl"><span class="c1"># Resume next day with full context!</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p><strong>Result</strong>: ‚úÖ <strong>No more &ldquo;conversation too long&rdquo; errors!</strong></p>
<p><strong>Test 2: Context Switching</strong></p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl"><span class="c1"># Morning: Work on Python with CodeLlama</span>
</span></span><span class="line"><span class="cl">ollama run codellama:7b
</span></span><span class="line"><span class="cl"><span class="c1"># Save context: /tmp/python-session.json</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Afternoon: Work on network config with DeepSeek</span>
</span></span><span class="line"><span class="cl">ollama run deepseek-r1:7b
</span></span><span class="line"><span class="cl"><span class="c1"># Save context: /tmp/network-session.json</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Evening: Resume Python session with ALL previous context</span>
</span></span><span class="line"><span class="cl"><span class="c1"># Load context: /tmp/python-session.json</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p><strong>Result</strong>: ‚úÖ <strong>Persistent conversations across sessions!</strong></p>
<p><strong>Test 3: Large Context Window</strong></p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="c1"># Test with 10,000 word document</span>
</span></span><span class="line"><span class="cl"><span class="n">response_time</span> <span class="o">=</span> <span class="mf">8.2</span>  <span class="c1"># seconds</span>
</span></span><span class="line"><span class="cl"><span class="n">context_retained</span> <span class="o">=</span> <span class="kc">True</span>  <span class="c1"># Full document in context!</span>
</span></span><span class="line"><span class="cl"><span class="n">external_service</span> <span class="o">=</span> <span class="kc">False</span>  <span class="c1"># All local, all private</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p><strong>Result</strong>: ‚úÖ <strong>No external limits!</strong></p>
<h2 id="300-pm---the-freedom-realization">3:00 PM - The Freedom Realization<a hidden class="anchor" aria-hidden="true" href="#300-pm---the-freedom-realization">#</a></h2>
<p>I ran the same test I&rsquo;d done with Claude Code that triggered this whole journey:</p>
<pre class="mermaid">Input: 3-hour debugging session about network automation
Messages: 87 back-and-forth exchanges
Context size: ~50K tokens

Claude Code result: &#34;Context window overflow. Start new chat?&#34;
Ollama result: &#34;Ready for message 88. Full context retained.&#34;</pre><p><strong>The breakthrough</strong>: I could continue conversations <strong>indefinitely</strong>.</p>
<h2 id="what-i-gained-vs-external-services">What I Gained (vs External Services)<a hidden class="anchor" aria-hidden="true" href="#what-i-gained-vs-external-services">#</a></h2>
<p><strong>Before (Claude Code/ChatGPT)</strong>:</p>
<ul>
<li>Hit context limit ‚Üí Lose all context ‚Üí Start over</li>
<li>Can&rsquo;t save/resume conversations</li>
<li>Dependent on service availability</li>
<li>Every conversation logged externally</li>
<li>Limited experimentation (don&rsquo;t want to hit limits)</li>
</ul>
<p><strong>After (Local Ollama)</strong>:</p>
<ul>
<li>Context limited only by hardware (configurable)</li>
<li>Save/resume any conversation anytime</li>
<li>Works offline, no service dependency</li>
<li>Complete privacy (never leaves machine)</li>
<li>Unlimited experimentation (iterate fearlessly)</li>
</ul>
<p><strong>Cost Comparison</strong> (bonus):</p>
<pre class="mermaid">Claude API (if I were using it): $720/year
Ollama (local): $0/year
RTX 4080 (already owned): Already paid for</pre><p>But <strong>cost wasn&rsquo;t the driver</strong> - <strong>freedom was</strong>.</p>
<h2 id="september-19-900-am---the-supervisor-pattern">September 19, 9:00 AM - The Supervisor Pattern<a hidden class="anchor" aria-hidden="true" href="#september-19-900-am---the-supervisor-pattern">#</a></h2>
<p>With 17 models available, I built an orchestrator to route tasks to the best model:</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">class</span> <span class="nc">ModelSupervisor</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">models</span> <span class="o">=</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">            <span class="s2">&#34;reasoning&#34;</span><span class="p">:</span> <span class="s2">&#34;deepseek-r1:7b&#34;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">            <span class="s2">&#34;general&#34;</span><span class="p">:</span> <span class="s2">&#34;mistral:7b-instruct&#34;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">            <span class="s2">&#34;code&#34;</span><span class="p">:</span> <span class="s2">&#34;codellama:7b&#34;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">            <span class="s2">&#34;fast&#34;</span><span class="p">:</span> <span class="s2">&#34;phi-3:mini&#34;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">            <span class="s2">&#34;creative&#34;</span><span class="p">:</span> <span class="s2">&#34;nous-hermes-2:latest&#34;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">            <span class="s2">&#34;long_context&#34;</span><span class="p">:</span> <span class="s2">&#34;llama3.1:8b&#34;</span>  <span class="c1"># 128K context!</span>
</span></span><span class="line"><span class="cl">        <span class="p">}</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">route_task</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">task_type</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">prompt</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="s2">&#34;&#34;&#34;Route task to optimal model.&#34;&#34;&#34;</span>
</span></span><span class="line"><span class="cl">        <span class="n">model</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">task_type</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">models</span><span class="p">[</span><span class="s2">&#34;general&#34;</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="n">response</span> <span class="o">=</span> <span class="n">requests</span><span class="o">.</span><span class="n">post</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">            <span class="s2">&#34;http://localhost:11434/api/generate&#34;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">            <span class="n">json</span><span class="o">=</span><span class="p">{</span><span class="s2">&#34;model&#34;</span><span class="p">:</span> <span class="n">model</span><span class="p">,</span> <span class="s2">&#34;prompt&#34;</span><span class="p">:</span> <span class="n">prompt</span><span class="p">}</span>
</span></span><span class="line"><span class="cl">        <span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="k">return</span> <span class="n">response</span><span class="o">.</span><span class="n">json</span><span class="p">()[</span><span class="s2">&#34;response&#34;</span><span class="p">]</span>
</span></span></code></pre></td></tr></table>
</div>
</div><pre class="mermaid">graph TD
    User[üë§ User Request] --&gt; Router{üéØ Task Router&lt;br/&gt;Model Supervisor}

    Router --&gt;|&#34;Need reasoning&lt;br/&gt;or analysis&#34;| DeepSeek[üß† DeepSeek R1 7B&lt;br/&gt;32K context&lt;br/&gt;4.2GB VRAM]
    Router --&gt;|&#34;Code generation&lt;br/&gt;or debugging&#34;| CodeLlama[üíª CodeLlama 7B&lt;br/&gt;16K context&lt;br/&gt;4.3GB VRAM]
    Router --&gt;|&#34;Quick question&lt;br/&gt;fast response&#34;| Mistral[‚ö° Mistral 7B&lt;br/&gt;32K context&lt;br/&gt;4.1GB VRAM]
    Router --&gt;|&#34;Long document&lt;br/&gt;large context&#34;| Llama[üÜï Llama 3.1 8B&lt;br/&gt;128K context!&lt;br/&gt;4.8GB VRAM]
    Router --&gt;|&#34;Heavy analysis&lt;br/&gt;complex task&#34;| Yi[üèãÔ∏è Yi-34B Quantized&lt;br/&gt;4K context&lt;br/&gt;12.1GB VRAM]

    DeepSeek --&gt; Response[üì§ Response with&lt;br/&gt;Full Context Retained]
    CodeLlama --&gt; Response
    Mistral --&gt; Response
    Llama --&gt; Response
    Yi --&gt; Response

    style Router fill:#ffd700,stroke:#ff6347,stroke-width:3px
    style Llama fill:#98fb98,stroke:#228b22,stroke-width:2px
    style Response fill:#e1f5fe,stroke:#1976d2,stroke-width:2px</pre><p><strong>Usage</strong>:</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">supervisor</span> <span class="o">=</span> <span class="n">ModelSupervisor</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Long context work ‚Üí Llama 3.1 (128K context)</span>
</span></span><span class="line"><span class="cl"><span class="n">analysis</span> <span class="o">=</span> <span class="n">supervisor</span><span class="o">.</span><span class="n">route_task</span><span class="p">(</span><span class="s2">&#34;long_context&#34;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="s2">&#34;Analyze this entire 100-page document...&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Code task ‚Üí CodeLlama</span>
</span></span><span class="line"><span class="cl"><span class="n">code_review</span> <span class="o">=</span> <span class="n">supervisor</span><span class="o">.</span><span class="n">route_task</span><span class="p">(</span><span class="s2">&#34;code&#34;</span><span class="p">,</span> <span class="s2">&#34;Review this function...&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Quick response ‚Üí Phi-3 Mini</span>
</span></span><span class="line"><span class="cl"><span class="n">quick_answer</span> <span class="o">=</span> <span class="n">supervisor</span><span class="o">.</span><span class="n">route_task</span><span class="p">(</span><span class="s2">&#34;fast&#34;</span><span class="p">,</span> <span class="s2">&#34;What is FastAPI?&#34;</span><span class="p">)</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>The system could now <strong>self-optimize</strong> based on context needs.</p>
<h2 id="what-worked">What Worked<a hidden class="anchor" aria-hidden="true" href="#what-worked">#</a></h2>
<p><strong>Ollama&rsquo;s Model Management</strong>:
Single command to pull, update, or remove models. No Docker containers, no config files, no complexity.</p>
<p><strong>Context Persistence</strong>:
Finally solved the original Day Zero problem - no more losing conversation history!</p>
<p><strong>GPU Performance</strong>:
RTX 4080 handled everything I threw at it. 16GB VRAM was the sweet spot for running multiple 7B models.</p>
<p><strong>Privacy &amp; Control</strong>:
All conversations stay local. No external logging. Complete ownership of my AI interactions.</p>
<p><strong>Freedom to Experiment</strong>:
No context limits = fearless iteration. Can explore ideas without worrying about hitting walls.</p>
<h2 id="what-still-sucked">What Still Sucked<a hidden class="anchor" aria-hidden="true" href="#what-still-sucked">#</a></h2>
<p><strong>Model Switching Latency</strong>:
Loading a new model: 2-4 seconds. Not terrible, but noticeable when switching frequently.</p>
<p><strong>VRAM Juggling</strong>:
Can&rsquo;t run Mixtral 8x7B (14.2GB) alongside anything else. Had to be strategic about which models stayed loaded.</p>
<p><strong>Quality Variance</strong>:
Some models (Phi-3 Mini) were fast but shallow. Others (DeepSeek R1) were brilliant but slower. Required testing to find the right fit.</p>
<p><strong>Still Need Claude Code</strong>:
Local models are good, but Claude Code&rsquo;s reasoning is still unmatched for complex tasks. Ollama complements, doesn&rsquo;t replace.</p>
<h2 id="the-numbers-15-hour-sprint">The Numbers (15-Hour Sprint)<a hidden class="anchor" aria-hidden="true" href="#the-numbers-15-hour-sprint">#</a></h2>
<table>
  <thead>
      <tr>
          <th>Metric</th>
          <th>Value</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td><strong>Implementation Time</strong></td>
          <td>15 hours (Sept 18-19)</td>
      </tr>
      <tr>
          <td><strong>Models Installed</strong></td>
          <td>17</td>
      </tr>
      <tr>
          <td><strong>Total Download Size</strong></td>
          <td>78GB</td>
      </tr>
      <tr>
          <td><strong>VRAM Available</strong></td>
          <td>16GB (RTX 4080)</td>
      </tr>
      <tr>
          <td><strong>Context Limit Freedom</strong></td>
          <td>Unlimited (hardware-bound)</td>
      </tr>
      <tr>
          <td><strong>Average Response Time</strong></td>
          <td>2.1 seconds</td>
      </tr>
      <tr>
          <td><strong>Concurrent Models</strong></td>
          <td>3 (12.4GB VRAM)</td>
      </tr>
      <tr>
          <td><strong>External Dependencies</strong></td>
          <td>Eliminated</td>
      </tr>
  </tbody>
</table>
<p><code>‚òÖ Insight ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ</code>
<strong>The Freedom of Local Inference:</strong></p>
<p>Switching to local LLMs wasn&rsquo;t about cost - it was about <strong>solving the original problem</strong>:</p>
<ol>
<li><strong>Context continuity</strong> - No more &ldquo;conversation too long&rdquo; errors</li>
<li><strong>Conversation persistence</strong> - Save/resume anytime</li>
<li><strong>Privacy</strong> - Conversations never leave the machine</li>
<li><strong>Offline capability</strong> - No internet required</li>
<li><strong>Experimentation freedom</strong> - Iterate without external limits</li>
<li><strong>Learning</strong> - Direct access to model internals, VRAM, performance tuning</li>
</ol>
<p>The cost savings ($0/year vs potential API costs) were a bonus. The real win was <strong>never hitting context limits again</strong>.</p>
<p>Local-first AI infrastructure isn&rsquo;t just cheaper - it&rsquo;s <strong>fundamentally different</strong>. You own your conversations. You control your context. You decide when to move on.</p>
<p><strong>Day Zero&rsquo;s context window problem? Finally solved.</strong>
<code>‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ</code></p>
<h2 id="what-i-learned">What I Learned<a hidden class="anchor" aria-hidden="true" href="#what-i-learned">#</a></h2>
<p><strong>1. The original problem drives the best solutions</strong>
Day Zero: Context window overflow. Episode 3: Local LLMs with persistent context. The solution directly addressed the root cause.</p>
<p><strong>2. Hardware limitations become features</strong>
16GB VRAM forced model selection discipline. Can&rsquo;t run everything = must choose the right tool for each task.</p>
<p><strong>3. Context persistence &gt; raw performance</strong>
A 7B local model you can save/resume beats a cloud model that forces restarts.</p>
<p><strong>4. Privacy enables experimentation</strong>
Knowing conversations stay local removes psychological barriers to trying wild ideas.</p>
<p><strong>5. Local doesn&rsquo;t mean isolated</strong>
Ollama + Claude Code = best of both worlds. Use local for persistent work, cloud for complex reasoning.</p>
<h2 id="whats-next">What&rsquo;s Next<a hidden class="anchor" aria-hidden="true" href="#whats-next">#</a></h2>
<p>Ollama was running. I could maintain conversations indefinitely. But the system was generating responses faster than I could organize them.</p>
<p>By September 22, I&rsquo;d have <strong>1,142 markdown files</strong> in the vault - including all these Ollama conversation logs.</p>
<p>By September 24, I&rsquo;d be drowning in documentation again.</p>
<p>By September 27, I&rsquo;d build automation to solve the organization problem&hellip; using these same local models.</p>
<p>But first, I needed to survive the documentation explosion.</p>
<hr>
<p><strong>Next Episode</strong>: &ldquo;Documentation Overload: When 1,142 Files Become Unmanageable&rdquo; - The moment persistent conversations created a new problem, and why I built ChromaDB indexing.</p>
<hr>
<p><em>This is Episode 3 of &ldquo;Season 1: From Zero to Automated Infrastructure&rdquo; - documenting the AI awakening that solved the context window problem.</em></p>
<p><em>Previous Episode</em>: <a href="season-1-episode-2-mvp-72-hours.md">Building the Foundation: MVP in 72 Hours</a>
<em>Complete Series</em>: <a href="/01-Inbox/BLOG-SERIES-SEASON-1-COMPLETE-MAPPING-2025-10-05.md">Season 1 Mapping Report</a></p>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
      <li><a href="https://blog.rduffy.uk/tags/ollama/">Ollama</a></li>
      <li><a href="https://blog.rduffy.uk/tags/local-llm/">Local-Llm</a></li>
      <li><a href="https://blog.rduffy.uk/tags/deepseek/">Deepseek</a></li>
      <li><a href="https://blog.rduffy.uk/tags/rtx-4080/">Rtx-4080</a></li>
      <li><a href="https://blog.rduffy.uk/tags/ai-infrastructure/">Ai-Infrastructure</a></li>
      <li><a href="https://blog.rduffy.uk/tags/context-windows/">Context-Windows</a></li>
    </ul>
<nav class="paginav">
  <a class="prev" href="https://blog.rduffy.uk/posts/2025-10-05-season-1-episode-1-day-zero/">
    <span class="title">¬´ Prev</span>
    <br>
    <span>Day Zero: The ConvoCanvas Vision</span>
  </a>
  <a class="next" href="https://blog.rduffy.uk/posts/test-post/">
    <span class="title">Next ¬ª</span>
    <br>
    <span>Testing My New Blog Publishing System</span>
  </a>
</nav>


<ul class="share-buttons">
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share The AI Awakening: Breaking Free from Context Limits on x"
            href="https://x.com/intent/tweet/?text=The%20AI%20Awakening%3a%20Breaking%20Free%20from%20Context%20Limits&amp;url=https%3a%2f%2fblog.rduffy.uk%2fposts%2f2025-10-05-season-1-episode-3-ai-awakening%2f&amp;hashtags=ollama%2clocal-llm%2cdeepseek%2crtx-4080%2cai-infrastructure%2ccontext-windows">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M512 62.554 L 512 449.446 C 512 483.97 483.97 512 449.446 512 L 62.554 512 C 28.03 512 0 483.97 0 449.446 L 0 62.554 C 0 28.03 28.029 0 62.554 0 L 449.446 0 C 483.971 0 512 28.03 512 62.554 Z M 269.951 190.75 L 182.567 75.216 L 56 75.216 L 207.216 272.95 L 63.9 436.783 L 125.266 436.783 L 235.9 310.383 L 332.567 436.783 L 456 436.783 L 298.367 228.367 L 432.367 75.216 L 371.033 75.216 Z M 127.633 110 L 164.101 110 L 383.481 400.065 L 349.5 400.065 Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share The AI Awakening: Breaking Free from Context Limits on linkedin"
            href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2fblog.rduffy.uk%2fposts%2f2025-10-05-season-1-episode-3-ai-awakening%2f&amp;title=The%20AI%20Awakening%3a%20Breaking%20Free%20from%20Context%20Limits&amp;summary=The%20AI%20Awakening%3a%20Breaking%20Free%20from%20Context%20Limits&amp;source=https%3a%2f%2fblog.rduffy.uk%2fposts%2f2025-10-05-season-1-episode-3-ai-awakening%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-288.985,423.278l0,-225.717l-75.04,0l0,225.717l75.04,0Zm270.539,0l0,-129.439c0,-69.333 -37.018,-101.586 -86.381,-101.586c-39.804,0 -57.634,21.891 -67.617,37.266l0,-31.958l-75.021,0c0.995,21.181 0,225.717 0,225.717l75.02,0l0,-126.056c0,-6.748 0.486,-13.492 2.474,-18.315c5.414,-13.475 17.767,-27.434 38.494,-27.434c27.135,0 38.007,20.707 38.007,51.037l0,120.768l75.024,0Zm-307.552,-334.556c-25.674,0 -42.448,16.879 -42.448,39.002c0,21.658 16.264,39.002 41.455,39.002l0.484,0c26.165,0 42.452,-17.344 42.452,-39.002c-0.485,-22.092 -16.241,-38.954 -41.943,-39.002Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share The AI Awakening: Breaking Free from Context Limits on reddit"
            href="https://reddit.com/submit?url=https%3a%2f%2fblog.rduffy.uk%2fposts%2f2025-10-05-season-1-episode-3-ai-awakening%2f&title=The%20AI%20Awakening%3a%20Breaking%20Free%20from%20Context%20Limits">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-3.446,265.638c0,-22.964 -18.616,-41.58 -41.58,-41.58c-11.211,0 -21.361,4.457 -28.841,11.666c-28.424,-20.508 -67.586,-33.757 -111.204,-35.278l18.941,-89.121l61.884,13.157c0.756,15.734 13.642,28.29 29.56,28.29c16.407,0 29.706,-13.299 29.706,-29.701c0,-16.403 -13.299,-29.702 -29.706,-29.702c-11.666,0 -21.657,6.792 -26.515,16.578l-69.105,-14.69c-1.922,-0.418 -3.939,-0.042 -5.585,1.036c-1.658,1.073 -2.811,2.761 -3.224,4.686l-21.152,99.438c-44.258,1.228 -84.046,14.494 -112.837,35.232c-7.468,-7.164 -17.589,-11.591 -28.757,-11.591c-22.965,0 -41.585,18.616 -41.585,41.58c0,16.896 10.095,31.41 24.568,37.918c-0.639,4.135 -0.99,8.328 -0.99,12.576c0,63.977 74.469,115.836 166.33,115.836c91.861,0 166.334,-51.859 166.334,-115.836c0,-4.218 -0.347,-8.387 -0.977,-12.493c14.564,-6.47 24.735,-21.034 24.735,-38.001Zm-119.474,108.193c-20.27,20.241 -59.115,21.816 -70.534,21.816c-11.428,0 -50.277,-1.575 -70.522,-21.82c-3.007,-3.008 -3.007,-7.882 0,-10.889c3.003,-2.999 7.882,-3.003 10.885,0c12.777,12.781 40.11,17.317 59.637,17.317c19.522,0 46.86,-4.536 59.657,-17.321c3.016,-2.999 7.886,-2.995 10.885,0.008c3.008,3.011 3.003,7.882 -0.008,10.889Zm-5.23,-48.781c-16.373,0 -29.701,-13.324 -29.701,-29.698c0,-16.381 13.328,-29.714 29.701,-29.714c16.378,0 29.706,13.333 29.706,29.714c0,16.374 -13.328,29.698 -29.706,29.698Zm-160.386,-29.702c0,-16.381 13.328,-29.71 29.714,-29.71c16.369,0 29.689,13.329 29.689,29.71c0,16.373 -13.32,29.693 -29.689,29.693c-16.386,0 -29.714,-13.32 -29.714,-29.693Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share The AI Awakening: Breaking Free from Context Limits on facebook"
            href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fblog.rduffy.uk%2fposts%2f2025-10-05-season-1-episode-3-ai-awakening%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-106.468,0l0,-192.915l66.6,0l12.672,-82.621l-79.272,0l0,-53.617c0,-22.603 11.073,-44.636 46.58,-44.636l36.042,0l0,-70.34c0,0 -32.71,-5.582 -63.982,-5.582c-65.288,0 -107.96,39.569 -107.96,111.204l0,62.971l-72.573,0l0,82.621l72.573,0l0,192.915l-191.104,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share The AI Awakening: Breaking Free from Context Limits on whatsapp"
            href="https://api.whatsapp.com/send?text=The%20AI%20Awakening%3a%20Breaking%20Free%20from%20Context%20Limits%20-%20https%3a%2f%2fblog.rduffy.uk%2fposts%2f2025-10-05-season-1-episode-3-ai-awakening%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-58.673,127.703c-33.842,-33.881 -78.847,-52.548 -126.798,-52.568c-98.799,0 -179.21,80.405 -179.249,179.234c-0.013,31.593 8.241,62.428 23.927,89.612l-25.429,92.884l95.021,-24.925c26.181,14.28 55.659,21.807 85.658,21.816l0.074,0c98.789,0 179.206,-80.413 179.247,-179.243c0.018,-47.895 -18.61,-92.93 -52.451,-126.81Zm-126.797,275.782l-0.06,0c-26.734,-0.01 -52.954,-7.193 -75.828,-20.767l-5.441,-3.229l-56.386,14.792l15.05,-54.977l-3.542,-5.637c-14.913,-23.72 -22.791,-51.136 -22.779,-79.287c0.033,-82.142 66.867,-148.971 149.046,-148.971c39.793,0.014 77.199,15.531 105.329,43.692c28.128,28.16 43.609,65.592 43.594,105.4c-0.034,82.149 -66.866,148.983 -148.983,148.984Zm81.721,-111.581c-4.479,-2.242 -26.499,-13.075 -30.604,-14.571c-4.105,-1.495 -7.091,-2.241 -10.077,2.241c-2.986,4.483 -11.569,14.572 -14.182,17.562c-2.612,2.988 -5.225,3.364 -9.703,1.12c-4.479,-2.241 -18.91,-6.97 -36.017,-22.23c-13.314,-11.876 -22.304,-26.542 -24.916,-31.026c-2.612,-4.484 -0.279,-6.908 1.963,-9.14c2.016,-2.007 4.48,-5.232 6.719,-7.847c2.24,-2.615 2.986,-4.484 4.479,-7.472c1.493,-2.99 0.747,-5.604 -0.374,-7.846c-1.119,-2.241 -10.077,-24.288 -13.809,-33.256c-3.635,-8.733 -7.327,-7.55 -10.077,-7.688c-2.609,-0.13 -5.598,-0.158 -8.583,-0.158c-2.986,0 -7.839,1.121 -11.944,5.604c-4.105,4.484 -15.675,15.32 -15.675,37.364c0,22.046 16.048,43.342 18.287,46.332c2.24,2.99 31.582,48.227 76.511,67.627c10.685,4.615 19.028,7.371 25.533,9.434c10.728,3.41 20.492,2.929 28.209,1.775c8.605,-1.285 26.499,-10.833 30.231,-21.295c3.732,-10.464 3.732,-19.431 2.612,-21.298c-1.119,-1.869 -4.105,-2.99 -8.583,-5.232Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share The AI Awakening: Breaking Free from Context Limits on telegram"
            href="https://telegram.me/share/url?text=The%20AI%20Awakening%3a%20Breaking%20Free%20from%20Context%20Limits&amp;url=https%3a%2f%2fblog.rduffy.uk%2fposts%2f2025-10-05-season-1-episode-3-ai-awakening%2f">
            <svg version="1.1" xml:space="preserve" viewBox="2 2 28 28" height="30px" width="30px" fill="currentColor">
                <path
                    d="M26.49,29.86H5.5a3.37,3.37,0,0,1-2.47-1,3.35,3.35,0,0,1-1-2.47V5.48A3.36,3.36,0,0,1,3,3,3.37,3.37,0,0,1,5.5,2h21A3.38,3.38,0,0,1,29,3a3.36,3.36,0,0,1,1,2.46V26.37a3.35,3.35,0,0,1-1,2.47A3.38,3.38,0,0,1,26.49,29.86Zm-5.38-6.71a.79.79,0,0,0,.85-.66L24.73,9.24a.55.55,0,0,0-.18-.46.62.62,0,0,0-.41-.17q-.08,0-16.53,6.11a.59.59,0,0,0-.41.59.57.57,0,0,0,.43.52l4,1.24,1.61,4.83a.62.62,0,0,0,.63.43.56.56,0,0,0,.4-.17L16.54,20l4.09,3A.9.9,0,0,0,21.11,23.15ZM13.8,20.71l-1.21-4q8.72-5.55,8.78-5.55c.15,0,.23,0,.23.16a.18.18,0,0,1,0,.06s-2.51,2.3-7.52,6.8Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share The AI Awakening: Breaking Free from Context Limits on ycombinator"
            href="https://news.ycombinator.com/submitlink?t=The%20AI%20Awakening%3a%20Breaking%20Free%20from%20Context%20Limits&u=https%3a%2f%2fblog.rduffy.uk%2fposts%2f2025-10-05-season-1-episode-3-ai-awakening%2f">
            <svg version="1.1" xml:space="preserve" width="30px" height="30px" viewBox="0 0 512 512" fill="currentColor"
                xmlns:inkscape="http://www.inkscape.org/namespaces/inkscape">
                <path
                    d="M449.446 0C483.971 0 512 28.03 512 62.554L512 449.446C512 483.97 483.97 512 449.446 512L62.554 512C28.03 512 0 483.97 0 449.446L0 62.554C0 28.03 28.029 0 62.554 0L449.446 0ZM183.8767 87.9921H121.8427L230.6673 292.4508V424.0079H281.3328V292.4508L390.1575 87.9921H328.1233L256 238.2489z" />
            </svg>
        </a>
    </li>
</ul>

  </footer>
</article>
    </main>
    
<footer class="footer">
        <span>&copy; 2025 <a href="https://blog.rduffy.uk/">Ryan Duffy - AI Infrastructure &amp; Local LLM Journey</a></span> ¬∑ 

    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a><script type="module" data-cfasync="false">
  
  if (document.querySelector('.mermaid')) {
    const mermaid = await import('https://cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.esm.min.mjs');
    mermaid.default.initialize({
      startOnLoad: true,
      theme: 'dark',
      themeVariables: {
        primaryColor: '#BB86FC',
        primaryTextColor: '#fff',
        primaryBorderColor: '#7c4dff',
        lineColor: '#F8B229',
        secondaryColor: '#03dac6',
        tertiaryColor: '#121212'
      }
    });
  }
</script>


<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
<script>
    document.querySelectorAll('pre > code').forEach((codeblock) => {
        const container = codeblock.parentNode.parentNode;

        const copybutton = document.createElement('button');
        copybutton.classList.add('copy-code');
        copybutton.innerHTML = 'copy';

        function copyingDone() {
            copybutton.innerHTML = 'copied!';
            setTimeout(() => {
                copybutton.innerHTML = 'copy';
            }, 2000);
        }

        copybutton.addEventListener('click', (cb) => {
            if ('clipboard' in navigator) {
                navigator.clipboard.writeText(codeblock.textContent);
                copyingDone();
                return;
            }

            const range = document.createRange();
            range.selectNodeContents(codeblock);
            const selection = window.getSelection();
            selection.removeAllRanges();
            selection.addRange(range);
            try {
                document.execCommand('copy');
                copyingDone();
            } catch (e) { };
            selection.removeRange(range);
        });

        if (container.classList.contains("highlight")) {
            container.appendChild(copybutton);
        } else if (container.parentNode.firstChild == container) {
            
        } else if (codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName == "TABLE") {
            
            codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(copybutton);
        } else {
            
            codeblock.parentNode.appendChild(copybutton);
        }
    });
</script>
</body>

</html>
