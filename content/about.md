---
title: "About"
url: "/about/"
ShowReadingTime: false
ShowShareButtons: false
ShowPostNavLinks: false
ShowBreadCrumbs: false
ShowCodeCopyButtons: false
ShowWordCount: false
ShowRssButtonInSectionTermList: false
disableShare: true
---

# About This Blog

Hi, I'm **Ryan Duffy**, and this is my digital garden where I document my journey building a production-grade AI infrastructure on consumer hardware.

## What I'm Building

I run a complete local AI stack on an RTX 4080:
- **11 local LLM models** via Ollama (Mistral, Qwen, DeepSeek)
- **vLLM server** for high-performance inference
- **Kubernetes observability** (Jaeger, OpenTelemetry, Kafka)
- **ChromaDB semantic search** with 504 indexed documents
- **Automated workflows** using Prefect

## Why This Blog?

Most AI content is either:
- Theoretical tutorials that don't show real performance
- Enterprise solutions requiring $10k/month budgets
- Hobby projects that don't scale

**I'm bridging the gap** - showing how to build professional AI infrastructure on a £1,500 GPU that delivers production-quality results.

## What You'll Learn

- **Real benchmarks**: Actual tokens/sec, VRAM usage, latency measurements
- **Configuration deep-dives**: The settings that actually matter
- **Automation patterns**: How to build systems that maintain themselves
- **Cost optimization**: Getting $0/month inference with enterprise quality

## My Stack

**Hardware**: RTX 4080 (16GB), i9-13900KF, 62GB RAM
**Software**: Ubuntu 22.04, K3s, Ollama 0.12.3, vLLM, PyTorch
**Workflow**: Obsidian vault → Prefect automation → ChromaDB indexing
**Philosophy**: Local-first, automation-driven, measurably fast

## Connect

- **GitHub**: [github.com/rduffyuk](https://github.com/rduffyuk)
- **RSS**: Subscribe to stay updated on new posts

---

*Last updated: October 2025*
